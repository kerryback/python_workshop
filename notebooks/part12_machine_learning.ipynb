{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-image: url('https://www.dropbox.com/scl/fi/wdrnuojbnjx6lgfekrx85/mcnair.jpg?rlkey=wcbaw5au7vh5vt1g5d5x7fw8f&dl=1'); background-size: cover; background-position: center; height: 300px; display: flex; align-items: center; justify-content: center; color: white; text-shadow: 2px 2px 4px rgba(0,0,0,0.7); margin-bottom: 20px; position: relative;\">\n",
    "  <h1 style=\"text-align: center; font-size: 2.5em; margin: 0;\">JGSB Python Workshop <br> Part 12: Machine Learning</h1>\n",
    "  <div style=\"position: absolute; bottom: 10px; left: 15px; font-size: 0.9em; color: white; text-shadow: 2px 2px 4px rgba(0,0,0,0.7);\">\n",
    "    Authored by Kerry Back\n",
    "  </div>\n",
    "  <div style=\"position: absolute; bottom: 10px; right: 15px; text-align: right; font-size: 0.9em; color: white; text-shadow: 2px 2px 4px rgba(0,0,0,0.7);\">\n",
    "    Rice University, 9/6/2025\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Summary: Machine Learning for Business Success\n\nMachine learning is transforming business decision-making across industries. This notebook introduced you to the fundamental concepts and practical applications that every business professional should understand.\n\n## Key Takeaways\n\n**1. Machine Learning Types:**\n- **Supervised Learning:** Predict outcomes using labeled historical data (regression, classification)\n- **Unsupervised Learning:** Discover patterns without predefined targets (clustering, dimensionality reduction)\n- Each type solves different business problems and requires different evaluation approaches\n\n**2. Business Applications:**\n- **Customer Analytics:** Lifetime value prediction, churn prevention, segmentation\n- **Risk Management:** Credit scoring, fraud detection, operational risk assessment\n- **Operations:** Demand forecasting, inventory optimization, quality control\n- **Marketing:** Targeted campaigns, recommendation systems, price optimization\n\n**3. Model Selection Strategy:**\n- Start simple with linear models for interpretability\n- Use ensemble methods (Random Forest) for robust performance\n- Consider neural networks for complex, nonlinear patterns\n- Always balance performance with business requirements\n\n**4. Success Factors:**\n- **Data Quality:** Clean, relevant, sufficient data is crucial\n- **Feature Engineering:** Domain expertise creates valuable features\n- **Proper Evaluation:** Use appropriate metrics and validation techniques\n- **Business Integration:** Align models with operational capabilities and constraints\n\n## Next Steps for Business Professionals\n\n**1. Build Domain Expertise:** Understand your business data and what drives outcomes\n**2. Start Small:** Begin with simple, high-impact use cases\n**3. Collaborate:** Work closely with data scientists and IT teams\n**4. Measure Impact:** Track business metrics, not just model metrics\n**5. Iterate:** Continuously improve models based on real-world performance\n\n## Tools and Resources\n\n**Python Libraries for Business ML:**\n- **scikit-learn:** Comprehensive machine learning library\n- **pandas:** Data manipulation and analysis\n- **matplotlib/seaborn:** Data visualization\n- **xgboost:** Advanced gradient boosting\n- **statsmodels:** Statistical modeling\n\n**Cloud ML Platforms:**\n- AWS SageMaker, Google Cloud AI, Azure ML\n- Auto-ML tools for business users\n- Pre-built APIs for common tasks\n\n## Final Advice\n\nMachine learning is a powerful tool, but it's not magic. Success requires:\n- Clear business objectives\n- Quality data and domain expertise  \n- Appropriate model selection and evaluation\n- Thoughtful integration into business processes\n- Continuous monitoring and improvement\n\nStart with problems where you have good data and clear success metrics. Build your confidence with simpler models before tackling complex challenges. Remember: a simple model that's actually used is infinitely more valuable than a complex model that sits unused.\n\nThe future belongs to organizations that can effectively combine human insight with machine intelligence to make better decisions faster.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Machine Learning Implementation Best Practices\n\nSuccessful machine learning projects require more than just algorithms. This section covers essential practices for deploying ML solutions in business environments.\n\n## Model Selection and Evaluation\n\n**Cross-Validation:** Always use cross-validation to get robust performance estimates\n```python\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\nprint(f\"CV Accuracy: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\n```\n\n**Hyperparameter Tuning:** Systematically optimize model parameters\n```python\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 10]}\ngrid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\n```\n\n**Feature Engineering:** Create meaningful features from raw data\n- Domain knowledge is crucial for feature creation\n- Consider interaction terms, polynomial features, and transformations\n- Use feature selection techniques to reduce dimensionality\n\n## Business Integration Considerations\n\n**Model Interpretability:** Choose models based on business requirements\n- **High Interpretability:** Logistic Regression, Decision Trees, Linear Models\n- **Medium Interpretability:** Random Forest (feature importance), Regularized Models\n- **Low Interpretability:** Neural Networks, SVM with RBF kernel\n\n**Performance vs. Interpretability Trade-off:**\n- Regulatory industries often require interpretable models\n- High-stakes decisions may need explainable predictions\n- Consider ensemble methods that balance both needs\n\n**Handling Class Imbalance:**\n- Use stratified sampling for train/test splits\n- Consider SMOTE for synthetic data generation\n- Adjust class weights in algorithms\n- Focus on precision, recall, and F1-score rather than accuracy\n\n## Deployment and Monitoring\n\n**Model Versioning:** Track model versions and performance over time\n**Data Drift Detection:** Monitor for changes in input data distribution\n**Performance Monitoring:** Set up alerts for model performance degradation\n**A/B Testing:** Compare new models against current production models\n\n## Ethical Considerations\n\n**Bias Detection:** Check for discriminatory patterns across demographic groups\n**Fairness Metrics:** Ensure equitable treatment across different populations\n**Privacy Protection:** Implement appropriate data protection measures\n**Transparency:** Document model decisions and limitations for stakeholders",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Exercise: Market Basket Analysis for Cross-Selling\n\nAnalyze customer purchase patterns to identify products that are frequently bought together, enabling targeted cross-selling and store layout optimization.\n\n**Your Task:**\n1. **Generate Transaction Data:** Create a dataset with 2,000 transactions containing:\n   - Transaction ID, customer ID, purchase date\n   - Products purchased (from 20 different products across 4 categories)\n   - Realistic shopping patterns (complementary products, seasonal effects)\n\n2. **Association Rule Mining:**\n   - Calculate support, confidence, and lift for product combinations\n   - Identify strong association rules (e.g., \"If customers buy X, they also buy Y\")\n   - Find the most profitable cross-selling opportunities\n\n3. **Customer Clustering with Purchase Behavior:**\n   - Create customer profiles based on category preferences\n   - Use clustering to identify distinct shopping patterns\n   - Analyze seasonal and demographic variations\n\n4. **Business Recommendations:**\n   - Suggest product placement strategies for physical stores\n   - Develop targeted recommendation algorithms for e-commerce\n   - Calculate potential revenue impact of cross-selling initiatives\n   - Create customer journey maps based on purchase sequences\n\n**Bonus:** Implement a collaborative filtering recommendation system to suggest products to customers based on similar customers' purchases.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Advanced visualization and business interpretation\n\n# PCA for dimensionality reduction and visualization\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_cluster_scaled)\n\nprint(\"ADVANCED ANALYSIS AND VISUALIZATION\")\nprint(\"=\"*50)\nprint(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\nprint(f\"Total variance explained by 2 components: {pca.explained_variance_ratio_.sum():.1%}\")\n\n# Create comprehensive visualization\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# PCA scatter plot with predicted clusters\nscatter = axes[0, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='tab10', alpha=0.7)\naxes[0, 0].set_title(f'Customer Clusters (PCA Visualization)')\naxes[0, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\naxes[0, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\nplt.colorbar(scatter, ax=axes[0, 0])\n\n# PCA scatter plot with true segments (for comparison)\ntrue_segment_colors = pd.Categorical(customer_behavior['true_segment']).codes\nscatter2 = axes[0, 1].scatter(X_pca[:, 0], X_pca[:, 1], c=true_segment_colors, cmap='tab10', alpha=0.7)\naxes[0, 1].set_title('True Customer Segments (PCA)')\naxes[0, 1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\naxes[0, 1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n\n# Cluster characteristics heatmap\ncluster_analysis_normalized = cluster_analysis.div(cluster_analysis.max(), axis=1)\nim = axes[0, 2].imshow(cluster_analysis_normalized.T, cmap='RdYlBu_r', aspect='auto')\naxes[0, 2].set_title('Cluster Characteristics (Normalized)')\naxes[0, 2].set_xticks(range(len(cluster_analysis.index)))\naxes[0, 2].set_xticklabels([f'Cluster {i}' for i in cluster_analysis.index])\naxes[0, 2].set_yticks(range(len(clustering_features)))\naxes[0, 2].set_yticklabels(clustering_features, fontsize=8)\nplt.colorbar(im, ax=axes[0, 2])\n\n# Customer value by cluster\ncluster_value = customer_behavior.groupby('predicted_cluster')['annual_spending'].agg(['mean', 'count'])\naxes[1, 0].bar(cluster_value.index, cluster_value['mean'], alpha=0.7, color='skyblue')\naxes[1, 0].set_title('Average Annual Spending by Cluster')\naxes[1, 0].set_xlabel('Cluster')\naxes[1, 0].set_ylabel('Annual Spending ($)')\n\n# Add count labels on bars\nfor i, (cluster, row) in enumerate(cluster_value.iterrows()):\n    axes[1, 0].text(i, row['mean'] + 50, f'n={row[\"count\"]}', ha='center', fontsize=10)\n\n# Purchase frequency by cluster\ncluster_freq = customer_behavior.groupby('predicted_cluster')['purchase_frequency'].mean()\naxes[1, 1].bar(cluster_freq.index, cluster_freq.values, alpha=0.7, color='lightgreen')\naxes[1, 1].set_title('Average Purchase Frequency by Cluster')\naxes[1, 1].set_xlabel('Cluster')\naxes[1, 1].set_ylabel('Annual Purchases')\n\n# Engagement metrics by cluster\nengagement_metrics = customer_behavior.groupby('predicted_cluster')[['email_engagement', 'mobile_app_usage']].mean()\nengagement_metrics.plot(kind='bar', ax=axes[1, 2], alpha=0.7)\naxes[1, 2].set_title('Engagement Metrics by Cluster')\naxes[1, 2].set_xlabel('Cluster')\naxes[1, 2].set_ylabel('Engagement Score')\naxes[1, 2].legend()\naxes[1, 2].tick_params(axis='x', rotation=0)\n\nplt.tight_layout()\nplt.show()\n\n# Business interpretation and marketing strategy\nprint(f\"\\nBUSINESS INTERPRETATION AND MARKETING STRATEGY\")\nprint(\"=\"*60)\n\n# Name clusters based on characteristics\ncluster_names = {}\ncluster_strategies = {}\n\nfor cluster_id in sorted(customer_behavior['predicted_cluster'].unique()):\n    cluster_data = cluster_analysis.loc[cluster_id]\n    \n    # Determine cluster characteristics\n    high_spending = cluster_data['annual_spending'] > cluster_analysis['annual_spending'].median()\n    high_frequency = cluster_data['purchase_frequency'] > cluster_analysis['purchase_frequency'].median()\n    high_engagement = cluster_data['email_engagement'] > cluster_analysis['email_engagement'].median()\n    high_loyalty = cluster_data['loyalty_years'] > cluster_analysis['loyalty_years'].median()\n    \n    # Name cluster based on characteristics\n    if high_spending and not high_frequency:\n        name = \"Premium Customers\"\n        strategy = \"Personalized luxury offerings, VIP service, exclusive events\"\n    elif high_frequency and cluster_data['annual_spending'] > 1000:\n        name = \"Frequent Buyers\"\n        strategy = \"Loyalty rewards, bulk discounts, early access to new products\"\n    elif high_engagement and cluster_data['annual_spending'] < 800:\n        name = \"Deal Seekers\"\n        strategy = \"Promotional campaigns, limited-time offers, price-based marketing\"\n    elif cluster_data['loyalty_years'] < 1:\n        name = \"New Customers\"\n        strategy = \"Onboarding campaigns, educational content, trial offers\"\n    else:\n        name = \"At-Risk Customers\"\n        strategy = \"Re-engagement campaigns, win-back offers, surveys for feedback\"\n    \n    cluster_names[cluster_id] = name\n    cluster_strategies[cluster_id] = strategy\n\n# Display cluster insights\nfor cluster_id in sorted(customer_behavior['predicted_cluster'].unique()):\n    cluster_size = (customer_behavior['predicted_cluster'] == cluster_id).sum()\n    cluster_data = cluster_analysis.loc[cluster_id]\n    \n    print(f\"\\nCluster {cluster_id}: {cluster_names[cluster_id]} ({cluster_size} customers)\")\n    print(f\"Characteristics:\")\n    print(f\"  • Annual Spending: ${cluster_data['annual_spending']:,.0f}\")\n    print(f\"  • Purchase Frequency: {cluster_data['purchase_frequency']:.1f} times/year\")\n    print(f\"  • Average Order Value: ${cluster_data['avg_order_value']:.0f}\")\n    print(f\"  • Email Engagement: {cluster_data['email_engagement']:.1%}\")\n    print(f\"  • Loyalty: {cluster_data['loyalty_years']:.1f} years\")\n    print(f\"Marketing Strategy: {cluster_strategies[cluster_id]}\")\n\n# Calculate business impact\ntotal_revenue = customer_behavior['annual_spending'].sum()\ncluster_revenue = customer_behavior.groupby('predicted_cluster')['annual_spending'].sum()\n\nprint(f\"\\nREVENUE IMPACT BY CLUSTER\")\nprint(\"=\"*40)\nfor cluster_id in sorted(customer_behavior['predicted_cluster'].unique()):\n    revenue = cluster_revenue[cluster_id]\n    revenue_pct = revenue / total_revenue * 100\n    print(f\"{cluster_names[cluster_id]}: ${revenue:,.0f} ({revenue_pct:.1f}% of total)\")\n\nprint(f\"\\nTotal Company Revenue: ${total_revenue:,.0f}\")\n\n# Recommendations\nprint(f\"\\nSTRATEGIC RECOMMENDATIONS\")\nprint(\"=\"*40)\nprint(f\"1. Focus retention efforts on high-value segments\")\nprint(f\"2. Develop targeted campaigns for each cluster\")\nprint(f\"3. Implement personalized product recommendations\")\nprint(f\"4. Create cluster-specific communication strategies\")\nprint(f\"5. Monitor cluster migration over time\")\nprint(f\"6. A/B test marketing messages within clusters\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Prepare data for clustering\n# Remove customer_id and true_segment for unsupervised learning\nclustering_features = ['annual_spending', 'purchase_frequency', 'avg_order_value', \n                      'website_visits', 'support_tickets', 'loyalty_years', \n                      'email_engagement', 'mobile_app_usage']\n\nX_cluster = customer_behavior[clustering_features].copy()\n\n# Standardize features for clustering\nscaler_cluster = StandardScaler()\nX_cluster_scaled = scaler_cluster.fit_transform(X_cluster)\n\nprint(\"CLUSTERING DATA PREPARATION\")\nprint(\"=\"*50)\nprint(f\"Features for clustering: {clustering_features}\")\nprint(f\"Data shape: {X_cluster.shape}\")\nprint(f\"Data has been standardized for clustering algorithms\")\n\n# Determine optimal number of clusters using elbow method and silhouette score\ndef find_optimal_clusters(X, max_k=10):\n    \"\"\"Find optimal number of clusters using multiple methods\"\"\"\n    \n    inertias = []\n    silhouette_scores = []\n    k_range = range(2, max_k + 1)\n    \n    for k in k_range:\n        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n        cluster_labels = kmeans.fit_predict(X)\n        \n        inertias.append(kmeans.inertia_)\n        silhouette_scores.append(silhouette_score(X, cluster_labels))\n    \n    return k_range, inertias, silhouette_scores\n\n# Find optimal clusters\nk_range, inertias, silhouette_scores = find_optimal_clusters(X_cluster_scaled, max_k=8)\n\nprint(f\"\\nCluster validation metrics:\")\nfor i, k in enumerate(k_range):\n    print(f\"k={k}: Inertia={inertias[i]:.2f}, Silhouette Score={silhouette_scores[i]:.3f}\")\n\n# Optimal k is usually where silhouette score is highest or elbow in inertia\noptimal_k = k_range[np.argmax(silhouette_scores)]\nprint(f\"\\nOptimal number of clusters (highest silhouette): {optimal_k}\")\n\n# Visualize cluster validation\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Elbow plot\naxes[0].plot(k_range, inertias, 'bo-')\naxes[0].set_title('Elbow Method for Optimal k')\naxes[0].set_xlabel('Number of Clusters (k)')\naxes[0].set_ylabel('Inertia')\naxes[0].grid(True)\n\n# Silhouette score plot\naxes[1].plot(k_range, silhouette_scores, 'ro-')\naxes[1].set_title('Silhouette Score for Different k')\naxes[1].set_xlabel('Number of Clusters (k)')\naxes[1].set_ylabel('Silhouette Score')\naxes[1].axvline(x=optimal_k, color='red', linestyle='--', alpha=0.7, label=f'Optimal k={optimal_k}')\naxes[1].legend()\naxes[1].grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Perform K-means clustering with optimal k\nkmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\ncluster_labels = kmeans_optimal.fit_predict(X_cluster_scaled)\n\n# Add cluster labels to original data\ncustomer_behavior['predicted_cluster'] = cluster_labels\n\nprint(f\"\\nCLUSTERING RESULTS (k={optimal_k})\")\nprint(\"=\"*50)\nprint(f\"Predicted cluster distribution:\")\nprint(pd.Series(cluster_labels).value_counts().sort_index())\n\n# Analyze cluster characteristics\ncluster_analysis = customer_behavior.groupby('predicted_cluster')[clustering_features].mean()\nprint(f\"\\nCluster characteristics (mean values):\")\nprint(cluster_analysis.round(2))\n\n# Compare with true segments\nprint(f\"\\nComparison with true segments:\")\ncomparison = pd.crosstab(customer_behavior['true_segment'], \n                        customer_behavior['predicted_cluster'], \n                        margins=True)\nprint(comparison)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Example: Customer Segmentation for Marketing Strategy\n# Use clustering to identify distinct customer groups for targeted marketing\n\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\n\ndef generate_customer_behavior_data(n_customers=1000):\n    \"\"\"Generate customer behavior data for segmentation\"\"\"\n    np.random.seed(42)\n    \n    # Create distinct customer segments with different behaviors\n    segment_sizes = [300, 250, 200, 150, 100]  # 5 natural segments\n    all_customers = []\n    \n    # Segment 1: High-value, low-frequency (Premium customers)\n    n1 = segment_sizes[0]\n    premium_customers = pd.DataFrame({\n        'annual_spending': np.random.normal(2500, 400, n1),\n        'purchase_frequency': np.random.normal(8, 2, n1),\n        'avg_order_value': np.random.normal(300, 50, n1),\n        'website_visits': np.random.normal(25, 5, n1),\n        'support_tickets': np.random.normal(1, 0.5, n1),\n        'loyalty_years': np.random.normal(4, 1, n1),\n        'email_engagement': np.random.normal(0.6, 0.1, n1),\n        'mobile_app_usage': np.random.normal(15, 3, n1),\n        'true_segment': 'Premium'\n    })\n    all_customers.append(premium_customers)\n    \n    # Segment 2: Frequent, moderate-value (Regular customers)\n    n2 = segment_sizes[1]\n    regular_customers = pd.DataFrame({\n        'annual_spending': np.random.normal(1200, 200, n2),\n        'purchase_frequency': np.random.normal(20, 4, n2),\n        'avg_order_value': np.random.normal(60, 15, n2),\n        'website_visits': np.random.normal(45, 8, n2),\n        'support_tickets': np.random.normal(2, 1, n2),\n        'loyalty_years': np.random.normal(2, 0.8, n2),\n        'email_engagement': np.random.normal(0.4, 0.1, n2),\n        'mobile_app_usage': np.random.normal(25, 5, n2),\n        'true_segment': 'Regular'\n    })\n    all_customers.append(regular_customers)\n    \n    # Segment 3: Price-sensitive, deal-hunters (Bargain hunters)\n    n3 = segment_sizes[2]\n    bargain_customers = pd.DataFrame({\n        'annual_spending': np.random.normal(600, 150, n3),\n        'purchase_frequency': np.random.normal(15, 3, n3),\n        'avg_order_value': np.random.normal(40, 10, n3),\n        'website_visits': np.random.normal(35, 6, n3),\n        'support_tickets': np.random.normal(1.5, 0.8, n3),\n        'loyalty_years': np.random.normal(1.5, 0.5, n3),\n        'email_engagement': np.random.normal(0.7, 0.1, n3),  # High engagement for deals\n        'mobile_app_usage': np.random.normal(20, 4, n3),\n        'true_segment': 'Bargain Hunter'\n    })\n    all_customers.append(bargain_customers)\n    \n    # Segment 4: New, exploring customers (Explorers)\n    n4 = segment_sizes[3]\n    explorer_customers = pd.DataFrame({\n        'annual_spending': np.random.normal(300, 100, n4),\n        'purchase_frequency': np.random.normal(5, 2, n4),\n        'avg_order_value': np.random.normal(60, 20, n4),\n        'website_visits': np.random.normal(15, 4, n4),\n        'support_tickets': np.random.normal(3, 1, n4),  # Need more help\n        'loyalty_years': np.random.normal(0.5, 0.3, n4),\n        'email_engagement': np.random.normal(0.3, 0.1, n4),\n        'mobile_app_usage': np.random.normal(10, 3, n4),\n        'true_segment': 'Explorer'\n    })\n    all_customers.append(explorer_customers)\n    \n    # Segment 5: Dormant customers (At risk)\n    n5 = segment_sizes[4]\n    dormant_customers = pd.DataFrame({\n        'annual_spending': np.random.normal(150, 50, n5),\n        'purchase_frequency': np.random.normal(2, 1, n5),\n        'avg_order_value': np.random.normal(75, 25, n5),\n        'website_visits': np.random.normal(5, 2, n5),\n        'support_tickets': np.random.normal(0.5, 0.3, n5),\n        'loyalty_years': np.random.normal(3, 1, n5),  # Long-time but inactive\n        'email_engagement': np.random.normal(0.1, 0.05, n5),\n        'mobile_app_usage': np.random.normal(2, 1, n5),\n        'true_segment': 'Dormant'\n    })\n    all_customers.append(dormant_customers)\n    \n    # Combine all segments\n    combined_df = pd.concat(all_customers, ignore_index=True)\n    \n    # Add some noise and ensure positive values\n    numeric_columns = ['annual_spending', 'purchase_frequency', 'avg_order_value', \n                      'website_visits', 'support_tickets', 'loyalty_years', \n                      'email_engagement', 'mobile_app_usage']\n    \n    for col in numeric_columns:\n        combined_df[col] = np.maximum(combined_df[col], 0)\n        if col == 'email_engagement':\n            combined_df[col] = np.clip(combined_df[col], 0, 1)\n    \n    # Add customer IDs\n    combined_df['customer_id'] = range(1, len(combined_df) + 1)\n    \n    # Shuffle the data\n    combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n    \n    return combined_df\n\n# Generate customer data\ncustomer_behavior = generate_customer_behavior_data(1000)\n\nprint(\"CUSTOMER SEGMENTATION ANALYSIS\")\nprint(\"=\"*50)\nprint(f\"Dataset shape: {customer_behavior.shape}\")\nprint(f\"\\nFirst few rows:\")\nprint(customer_behavior.head())\n\nprint(f\"\\nTrue segment distribution:\")\nprint(customer_behavior['true_segment'].value_counts())\n\nprint(f\"\\nDataset statistics:\")\nprint(customer_behavior.describe())",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Unsupervised Learning: Finding Hidden Patterns\n\nUnsupervised learning discovers patterns in data without labeled examples. This is valuable for exploration, segmentation, and understanding the underlying structure of business data when you don't know what you're looking for.\n\n**Key Concepts:**\n- **No Target Variable:** Algorithms find patterns without being told what to look for\n- **Pattern Discovery:** Identify hidden relationships, groups, and structures\n- **Dimensionality Reduction:** Simplify complex data while preserving important information\n- **Anomaly Detection:** Find unusual observations that might indicate problems or opportunities\n\n**Main Types of Unsupervised Learning:**\n\n**1. Clustering:** Group similar observations together\n- **K-Means:** Partition data into k clusters based on similarity\n- **Hierarchical Clustering:** Build tree-like cluster structures\n- **DBSCAN:** Find clusters of varying shapes and identify outliers\n\n**2. Dimensionality Reduction:** Reduce the number of variables while preserving information\n- **Principal Component Analysis (PCA):** Find the most important directions of variation\n- **t-SNE:** Visualize high-dimensional data in 2D or 3D\n\n**3. Association Rules:** Find relationships between different items or events\n- **Market Basket Analysis:** \"People who buy X also buy Y\"\n\n**Business Applications:**\n- **Customer Segmentation:** Group customers by behavior, preferences, or value\n- **Market Research:** Identify distinct market segments and positioning opportunities\n- **Fraud Detection:** Find unusual transactions that deviate from normal patterns\n- **Recommendation Systems:** Suggest products based on similar customer preferences\n- **Operational Efficiency:** Identify process improvements and cost reduction opportunities\n- **Risk Management:** Detect anomalous patterns that might indicate problems",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Exercise: Credit Risk Assessment\n\nBuild a classification model to assess loan default risk for a financial institution. This exercise combines multiple business concepts including risk management, regulatory compliance, and profitability analysis.\n\n**Your Task:**\n1. **Generate Credit Dataset:** Create a dataset with 1,500 loan applications containing:\n   - Applicant demographics: age, income, employment_years, education_level\n   - Financial metrics: debt_to_income_ratio, credit_score, loan_amount, existing_loans\n   - Loan characteristics: loan_purpose (Auto/Home/Personal), term_months\n   - Target: loan_default (binary: 0=repaid, 1=defaulted)\n\n2. **Data Analysis:**\n   - Explore default rates by different customer segments\n   - Identify key risk factors through visualization\n   - Handle any class imbalance in the target variable\n\n3. **Model Development:**\n   - Compare Logistic Regression, Decision Tree, Random Forest, and SVM\n   - Use appropriate evaluation metrics for imbalanced classification\n   - Tune hyperparameters using cross-validation\n\n4. **Business Application:**\n   - Create a risk scoring system (Low/Medium/High risk segments)\n   - Calculate the cost of false positives (rejected good customers) vs false negatives (approved bad loans)\n   - Recommend approval thresholds based on business objectives\n   - Estimate the impact on loan portfolio profitability\n\n**Bonus:** Implement LIME (Local Interpretable Model-agnostic Explanations) to explain individual loan decisions for regulatory compliance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Detailed model analysis and business insights\n\n# Best performing model\nbest_classifier = classification_results[2]  # Random Forest typically performs best\nbest_model = best_classifier['model']\nbest_predictions = best_classifier['predictions']\nbest_probabilities = best_classifier['probabilities']\n\n# Confusion Matrix Analysis\ncm = confusion_matrix(y_test_churn, best_predictions)\ntn, fp, fn, tp = cm.ravel()\n\nprint(\"\\nDETAILED BUSINESS ANALYSIS\")\nprint(\"=\"*50)\nprint(f\"Best Model: {best_classifier['model_name']}\")\nprint(f\"Test Accuracy: {best_classifier['test_accuracy']:.1%}\")\nprint(f\"AUC Score: {best_classifier['auc']:.3f}\")\n\nprint(f\"\\nConfusion Matrix Analysis:\")\nprint(f\"True Negatives (Correctly predicted retained): {tn}\")\nprint(f\"False Positives (Incorrectly predicted churn): {fp}\")\nprint(f\"False Negatives (Missed churners): {fn}\")\nprint(f\"True Positives (Correctly predicted churn): {tp}\")\n\n# Business impact analysis\ntotal_customers = len(y_test_churn)\nactual_churners = y_test_churn.sum()\npredicted_churners = best_predictions.sum()\n\nprint(f\"\\nBusiness Impact Analysis:\")\nprint(f\"Total test customers: {total_customers}\")\nprint(f\"Actual churners: {actual_churners}\")\nprint(f\"Predicted churners: {predicted_churners}\")\nprint(f\"Precision: {tp/(tp+fp):.1%} (of predicted churners, how many actually churned)\")\nprint(f\"Recall: {tp/(tp+fn):.1%} (of actual churners, how many we caught)\")\n\n# ROI calculation (assuming intervention costs and retention value)\nintervention_cost = 50  # Cost to intervene per customer\nretention_value = 500  # Value of retaining a customer\n\n# Calculate ROI for different scenarios\nprint(f\"\\nROI Analysis (Intervention cost: ${intervention_cost}, Retention value: ${retention_value}):\")\n\n# Scenario 1: No model (random intervention)\nrandom_intervention_cost = (actual_churners / total_customers) * total_customers * intervention_cost\nrandom_retention = actual_churners * 0.3  # Assume 30% success rate without targeting\nrandom_value = random_retention * retention_value\nrandom_roi = (random_value - random_intervention_cost) / random_intervention_cost\n\nprint(f\"Random intervention ROI: {random_roi:.1%}\")\n\n# Scenario 2: With ML model\nml_intervention_cost = predicted_churners * intervention_cost\nml_retention = tp * 0.7  # Assume 70% success rate with targeted intervention\nml_value = ml_retention * retention_value\nml_roi = (ml_value - ml_intervention_cost) / ml_intervention_cost if ml_intervention_cost > 0 else 0\n\nprint(f\"ML-guided intervention ROI: {ml_roi:.1%}\")\nprint(f\"ROI improvement: {ml_roi - random_roi:.1%} percentage points\")\n\n# Feature importance analysis\nif hasattr(best_model, 'feature_importances_'):\n    feature_importance_churn = pd.DataFrame({\n        'Feature': churn_features,\n        'Importance': best_model.feature_importances_\n    }).sort_values('Importance', ascending=False)\n    \n    print(f\"\\nFeature Importance (Random Forest):\")\n    print(\"=\"*40)\n    for i, row in feature_importance_churn.head(5).iterrows():\n        print(f\"{row['Feature']:<25}: {row['Importance']:.3f}\")\n\n# Visualize classification results\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n# Model performance comparison\nmodels_class = classification_df['Model'].tolist()\nauc_scores = classification_df['AUC'].tolist()\n\naxes[0, 0].bar(models_class, auc_scores, alpha=0.7, color=['blue', 'green', 'red', 'orange'])\naxes[0, 0].set_title('Model Performance (AUC Score)')\naxes[0, 0].set_ylabel('AUC Score')\naxes[0, 0].tick_params(axis='x', rotation=45)\naxes[0, 0].set_ylim(0, 1)\n\n# ROC Curves\nfor result in classification_results:\n    fpr, tpr, _ = roc_curve(y_test_churn, result['probabilities'])\n    axes[0, 1].plot(fpr, tpr, label=f\"{result['model_name']} (AUC={result['auc']:.3f})\")\n\naxes[0, 1].plot([0, 1], [0, 1], 'k--', alpha=0.5)\naxes[0, 1].set_title('ROC Curves')\naxes[0, 1].set_xlabel('False Positive Rate')\naxes[0, 1].set_ylabel('True Positive Rate')\naxes[0, 1].legend()\n\n# Confusion Matrix Heatmap\ncm_df = pd.DataFrame(cm, index=['Retained', 'Churned'], columns=['Predicted Retained', 'Predicted Churned'])\nsns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])\naxes[1, 0].set_title(f'Confusion Matrix: {best_classifier[\"model_name\"]}')\n\n# Feature Importance Plot\nif hasattr(best_model, 'feature_importances_'):\n    top_features = feature_importance_churn.head(8)\n    axes[1, 1].barh(top_features['Feature'], top_features['Importance'], alpha=0.7)\n    axes[1, 1].set_title('Top Feature Importance')\n    axes[1, 1].set_xlabel('Importance Score')\n\nplt.tight_layout()\nplt.show()\n\n# Customer segmentation by churn probability\nprint(f\"\\nCUSTOMER RISK SEGMENTATION\")\nprint(\"=\"*50)\n\n# Create risk segments based on predicted probabilities\nrisk_thresholds = [0.3, 0.7]\nrisk_labels = ['Low Risk', 'Medium Risk', 'High Risk']\n\ntest_customers = pd.DataFrame({\n    'actual_churn': y_test_churn,\n    'predicted_prob': best_probabilities\n})\n\ntest_customers['risk_segment'] = pd.cut(test_customers['predicted_prob'], \n                                       bins=[0] + risk_thresholds + [1.0], \n                                       labels=risk_labels)\n\n# Analyze segments\nsegment_analysis = test_customers.groupby('risk_segment').agg({\n    'actual_churn': ['count', 'sum', 'mean']\n}).round(3)\n\nsegment_analysis.columns = ['Customer_Count', 'Actual_Churners', 'Churn_Rate']\nprint(segment_analysis)\n\nprint(f\"\\nBusiness Recommendations:\")\nprint(f\"• Focus retention efforts on High Risk customers ({segment_analysis.loc['High Risk', 'Customer_Count']} customers)\")\nprint(f\"• High Risk segment has {segment_analysis.loc['High Risk', 'Churn_Rate']:.1%} actual churn rate\")\nprint(f\"• Monitor Medium Risk customers for early warning signs\")\nprint(f\"• Low Risk customers require minimal intervention\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Prepare data for classification\ndef prepare_classification_data(df):\n    \"\"\"Prepare churn data for classification\"\"\"\n    \n    # Create dummy variables for contract type\n    df_ml = pd.get_dummies(df, columns=['contract_type'], prefix='contract')\n    \n    # Define features\n    feature_columns = [\n        'age', 'income', 'months_subscribed', 'monthly_charges',\n        'monthly_usage_hours', 'support_tickets', 'has_premium',\n        'auto_pay', 'paperless_billing',\n        'contract_Month-to-month', 'contract_One year', 'contract_Two year'\n    ]\n    \n    X = df_ml[feature_columns]\n    y = df_ml['churned']\n    \n    return X, y, feature_columns\n\n# Prepare classification data\nX_churn, y_churn, churn_features = prepare_classification_data(churn_data)\n\n# Split data\nX_train_churn, X_test_churn, y_train_churn, y_test_churn = train_test_split(\n    X_churn, y_churn, test_size=0.2, random_state=42, stratify=y_churn\n)\n\n# Scale features\nscaler_churn = StandardScaler()\nX_train_churn_scaled = scaler_churn.fit_transform(X_train_churn)\nX_test_churn_scaled = scaler_churn.transform(X_test_churn)\n\nprint(\"CLASSIFICATION DATA PREPARATION\")\nprint(\"=\"*50)\nprint(f\"Features shape: {X_churn.shape}\")\nprint(f\"Training set: {X_train_churn.shape[0]} customers\")\nprint(f\"Test set: {X_test_churn.shape[0]} customers\")\n\nprint(f\"\\nChurn rate in training set: {y_train_churn.mean():.1%}\")\nprint(f\"Churn rate in test set: {y_test_churn.mean():.1%}\")\n\n# Train classification models\ndef evaluate_classifier(model, X_train, X_test, y_train, y_test, model_name):\n    \"\"\"Train and evaluate classification model\"\"\"\n    \n    # Train model\n    model.fit(X_train, y_train)\n    \n    # Predictions\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n    \n    # Probability predictions (for ROC curve)\n    if hasattr(model, 'predict_proba'):\n        y_test_proba = model.predict_proba(X_test)[:, 1]\n    else:\n        y_test_proba = model.decision_function(X_test)\n    \n    # Calculate metrics\n    train_accuracy = accuracy_score(y_train, y_train_pred)\n    test_accuracy = accuracy_score(y_test, y_test_pred)\n    \n    precision = precision_score(y_test, y_test_pred)\n    recall = recall_score(y_test, y_test_pred)\n    f1 = f1_score(y_test, y_test_pred)\n    auc = roc_auc_score(y_test, y_test_proba)\n    \n    return {\n        'model_name': model_name,\n        'train_accuracy': train_accuracy,\n        'test_accuracy': test_accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1,\n        'auc': auc,\n        'model': model,\n        'predictions': y_test_pred,\n        'probabilities': y_test_proba\n    }\n\n# Initialize classification models\nclassifiers = {\n    'Logistic Regression': LogisticRegression(random_state=42),\n    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),\n    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n    'SVM': SVC(probability=True, random_state=42)\n}\n\n# Train and evaluate models\nclassification_results = []\n\nprint(\"\\nCLASSIFICATION MODEL TRAINING\")\nprint(\"=\"*50)\n\nfor name, model in classifiers.items():\n    print(f\"Training {name}...\")\n    \n    # Use scaled data for SVM and Logistic Regression\n    if name in ['SVM', 'Logistic Regression']:\n        X_train_use = X_train_churn_scaled\n        X_test_use = X_test_churn_scaled\n    else:\n        X_train_use = X_train_churn\n        X_test_use = X_test_churn\n    \n    result = evaluate_classifier(model, X_train_use, X_test_use, \n                                y_train_churn, y_test_churn, name)\n    classification_results.append(result)\n\n# Create results summary\nclassification_df = pd.DataFrame([\n    {\n        'Model': r['model_name'],\n        'Train Accuracy': r['train_accuracy'],\n        'Test Accuracy': r['test_accuracy'],\n        'Precision': r['precision'],\n        'Recall': r['recall'],\n        'F1-Score': r['f1_score'],\n        'AUC': r['auc'],\n        'Overfitting': r['train_accuracy'] - r['test_accuracy']\n    }\n    for r in classification_results\n])\n\nprint(\"\\nCLASSIFICATION RESULTS SUMMARY\")\nprint(\"=\"*90)\nprint(classification_df.round(4))",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Classification Example: Customer Churn Prediction\n# Predict which customers are likely to cancel their subscription\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\ndef generate_churn_data(n_customers=2000):\n    \"\"\"Generate realistic customer churn dataset\"\"\"\n    np.random.seed(42)\n    \n    # Customer demographics\n    age = np.random.normal(35, 12, n_customers)\n    age = np.clip(age, 18, 75)\n    \n    income = np.random.lognormal(10.8, 0.4, n_customers)\n    income = np.clip(income, 30000, 150000)\n    \n    # Service characteristics\n    months_subscribed = np.random.exponential(18, n_customers)\n    months_subscribed = np.clip(months_subscribed, 1, 60)\n    \n    monthly_charges = np.random.gamma(3, 25, n_customers)\n    monthly_charges = np.clip(monthly_charges, 20, 200)\n    \n    # Usage patterns\n    monthly_usage_hours = np.random.gamma(2, 15, n_customers)\n    support_tickets = np.random.poisson(2, n_customers)\n    \n    # Service features\n    has_premium = np.random.binomial(1, 0.3, n_customers)\n    auto_pay = np.random.binomial(1, 0.6, n_customers)\n    paperless_billing = np.random.binomial(1, 0.7, n_customers)\n    \n    # Contract type\n    contract_types = np.random.choice(['Month-to-month', 'One year', 'Two year'], \n                                    n_customers, p=[0.5, 0.3, 0.2])\n    \n    # Calculate churn probability based on business logic\n    churn_prob = (\n        0.1 +  # Base churn rate\n        0.001 * (45 - age) +  # Younger customers churn more\n        -0.000005 * income +  # Higher income customers churn less\n        -0.01 * months_subscribed +  # Longer tenure = less churn\n        0.002 * monthly_charges +  # Higher charges = more churn\n        -0.005 * monthly_usage_hours +  # Higher usage = less churn\n        0.05 * support_tickets +  # More tickets = more churn\n        -0.1 * has_premium +  # Premium customers churn less\n        -0.08 * auto_pay +  # Auto-pay customers churn less\n        np.where(contract_types == 'Month-to-month', 0.2,\n                np.where(contract_types == 'One year', 0.05, -0.05))  # Contract effect\n    )\n    \n    # Ensure probabilities are between 0 and 1\n    churn_prob = np.clip(churn_prob, 0, 1)\n    \n    # Generate actual churn based on probabilities\n    churned = np.random.binomial(1, churn_prob, n_customers)\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'customer_id': range(1, n_customers + 1),\n        'age': age,\n        'income': income,\n        'months_subscribed': months_subscribed,\n        'monthly_charges': monthly_charges,\n        'monthly_usage_hours': monthly_usage_hours,\n        'support_tickets': support_tickets,\n        'has_premium': has_premium,\n        'auto_pay': auto_pay,\n        'paperless_billing': paperless_billing,\n        'contract_type': contract_types,\n        'churned': churned\n    })\n    \n    return df\n\n# Generate churn data\nchurn_data = generate_churn_data(2000)\n\nprint(\"CUSTOMER CHURN PREDICTION\")\nprint(\"=\"*50)\nprint(f\"Dataset shape: {churn_data.shape}\")\nprint(f\"\\nChurn distribution:\")\nchurn_counts = churn_data['churned'].value_counts()\nprint(f\"Retained customers: {churn_counts[0]} ({churn_counts[0]/len(churn_data):.1%})\")\nprint(f\"Churned customers: {churn_counts[1]} ({churn_counts[1]/len(churn_data):.1%})\")\n\nprint(f\"\\nFirst few rows:\")\nprint(churn_data.head())\n\n# Analyze churn patterns\nprint(f\"\\nChurn rate by contract type:\")\nchurn_by_contract = churn_data.groupby('contract_type')['churned'].agg(['count', 'sum', 'mean'])\nchurn_by_contract['churn_rate'] = churn_by_contract['mean']\nprint(churn_by_contract[['count', 'sum', 'churn_rate']])\n\n# Visualize churn patterns\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# Churn distribution\nchurn_counts.plot(kind='bar', ax=axes[0, 0], color=['lightblue', 'lightcoral'])\naxes[0, 0].set_title('Churn Distribution')\naxes[0, 0].set_ylabel('Number of Customers')\naxes[0, 0].set_xticklabels(['Retained', 'Churned'], rotation=0)\n\n# Age distribution by churn\nchurn_data.boxplot(column='age', by='churned', ax=axes[0, 1])\naxes[0, 1].set_title('Age Distribution by Churn Status')\naxes[0, 1].set_xlabel('Churned (0=No, 1=Yes)')\n\n# Monthly charges by churn\nchurn_data.boxplot(column='monthly_charges', by='churned', ax=axes[0, 2])\naxes[0, 2].set_title('Monthly Charges by Churn Status')\naxes[0, 2].set_xlabel('Churned (0=No, 1=Yes)')\n\n# Tenure by churn\nchurn_data.boxplot(column='months_subscribed', by='churned', ax=axes[1, 0])\naxes[1, 0].set_title('Tenure by Churn Status')\naxes[1, 0].set_xlabel('Churned (0=No, 1=Yes)')\n\n# Contract type vs churn\ncontract_churn = churn_data.groupby(['contract_type', 'churned']).size().unstack()\ncontract_churn.plot(kind='bar', ax=axes[1, 1], color=['lightblue', 'lightcoral'])\naxes[1, 1].set_title('Churn by Contract Type')\naxes[1, 1].set_ylabel('Number of Customers')\naxes[1, 1].legend(['Retained', 'Churned'])\naxes[1, 1].tick_params(axis='x', rotation=45)\n\n# Support tickets vs churn\nsupport_churn = churn_data.groupby(['support_tickets', 'churned']).size().unstack(fill_value=0)\nsupport_churn.plot(kind='bar', ax=axes[1, 2], color=['lightblue', 'lightcoral'])\naxes[1, 2].set_title('Churn by Support Tickets')\naxes[1, 2].set_ylabel('Number of Customers')\naxes[1, 2].legend(['Retained', 'Churned'])\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Supervised Learning: Classification for Business Decisions\n\nClassification predicts **categorical outcomes** rather than continuous values. This is essential for business decisions like customer segmentation, risk assessment, quality control, and marketing targeting.\n\n**Key Concepts:**\n- **Binary Classification:** Two outcomes (Buy/Don't Buy, Approve/Reject, Pass/Fail)\n- **Multi-class Classification:** Multiple outcomes (High/Medium/Low Risk, Customer Segments)\n- **Probability Estimates:** Models often provide confidence scores for decisions\n- **Class Imbalance:** When some outcomes are much rarer than others (fraud detection)\n\n**Business Applications:**\n- **Customer Churn:** Will a customer cancel their subscription?\n- **Credit Risk:** Should we approve this loan application?\n- **Marketing Response:** Will a customer respond to this campaign?\n- **Quality Control:** Is this product defective?\n- **Employee Retention:** Is this employee likely to quit?\n- **Medical Diagnosis:** Does this patient have a specific condition?\n\n**Classification Algorithms:**\n- **Logistic Regression:** Linear boundaries with probability outputs\n- **Decision Trees:** Rule-based decisions that are easy to interpret\n- **Random Forest:** Combines many decision trees for better accuracy\n- **Support Vector Machines:** Find optimal boundaries between classes\n- **Neural Networks:** Can learn complex nonlinear decision boundaries\n\n**Evaluation Metrics:**\n- **Accuracy:** Percentage of correct predictions (can be misleading with imbalanced data)\n- **Precision:** Of predicted positives, how many were actually positive?\n- **Recall (Sensitivity):** Of actual positives, how many did we correctly identify?\n- **F1-Score:** Harmonic mean of precision and recall\n- **ROC Curve & AUC:** Trade-off between true positive rate and false positive rate",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Exercise: Sales Revenue Forecasting\n\nCreate a machine learning model to predict monthly sales revenue for a retail chain based on store characteristics, marketing spend, and seasonal factors.\n\n**Your Task:**\n1. **Generate Dataset:** Create a synthetic dataset with 500 stores containing:\n   - Store size (sq ft), age (years), location type (Mall/Street/Outlet)\n   - Monthly marketing spend, local competition score (1-10)\n   - Seasonal month (1-12), local population, median income\n   - Target: Monthly revenue (based on realistic business relationships)\n\n2. **Data Preparation:**\n   - Handle categorical variables (location type)\n   - Create seasonal features (sin/cos transformations for cyclical patterns)\n   - Split into train/test sets (80/20)\n   - Scale features appropriately\n\n3. **Model Comparison:**\n   - Train Linear, Ridge, Lasso, and Random Forest models\n   - Use cross-validation to tune hyperparameters\n   - Compare performance using MAE, RMSE, and R²\n\n4. **Business Analysis:**\n   - Identify the most important revenue drivers\n   - Calculate the ROI of marketing spend from the model\n   - Provide actionable insights for store management\n\n**Bonus:** Add interaction terms between marketing spend and store size to capture synergy effects.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Train and compare multiple regression models\n\ndef evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n    \"\"\"Train model and calculate performance metrics\"\"\"\n    \n    # Train the model\n    model.fit(X_train, y_train)\n    \n    # Make predictions\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n    \n    # Calculate metrics\n    train_mae = mean_absolute_error(y_train, y_train_pred)\n    test_mae = mean_absolute_error(y_test, y_test_pred)\n    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n    train_r2 = r2_score(y_train, y_train_pred)\n    test_r2 = r2_score(y_test, y_test_pred)\n    \n    return {\n        'model_name': model_name,\n        'train_mae': train_mae,\n        'test_mae': test_mae,\n        'train_rmse': train_rmse,\n        'test_rmse': test_rmse,\n        'train_r2': train_r2,\n        'test_r2': test_r2,\n        'model': model,\n        'predictions': y_test_pred\n    }\n\n# Initialize models\nmodels = {\n    'Linear Regression': LinearRegression(),\n    'Ridge Regression': Ridge(alpha=1.0),\n    'Lasso Regression': Lasso(alpha=1.0),\n    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)\n}\n\n# Train and evaluate models\nresults = []\npredictions = {}\n\nprint(\"MODEL TRAINING AND EVALUATION\")\nprint(\"=\"*50)\n\nfor name, model in models.items():\n    print(f\"Training {name}...\")\n    \n    # Use scaled data for regularized models, original data for Random Forest\n    if name in ['Ridge Regression', 'Lasso Regression']:\n        X_train_use = X_train_scaled\n        X_test_use = X_test_scaled\n    else:\n        X_train_use = X_train\n        X_test_use = X_test\n    \n    result = evaluate_model(model, X_train_use, X_test_use, y_train, y_test, name)\n    results.append(result)\n    predictions[name] = result['predictions']\n\n# Create results DataFrame\nresults_df = pd.DataFrame([\n    {\n        'Model': r['model_name'],\n        'Train MAE': r['train_mae'],\n        'Test MAE': r['test_mae'],\n        'Train RMSE': r['train_rmse'],\n        'Test RMSE': r['test_rmse'],\n        'Train R²': r['train_r2'],\n        'Test R²': r['test_r2'],\n        'Overfitting': r['train_r2'] - r['test_r2']\n    }\n    for r in results\n])\n\nprint(\"\\nMODEL PERFORMANCE COMPARISON\")\nprint(\"=\"*80)\nprint(results_df.round(4))\n\n# Visualize results\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n# Test R² comparison\nmodels_list = results_df['Model'].tolist()\ntest_r2_list = results_df['Test R²'].tolist()\n\naxes[0, 0].bar(models_list, test_r2_list, alpha=0.7, color=['blue', 'green', 'red', 'orange'])\naxes[0, 0].set_title('Model Performance (Test R²)')\naxes[0, 0].set_ylabel('R² Score')\naxes[0, 0].tick_params(axis='x', rotation=45)\n\n# Overfitting analysis\ntrain_r2_list = results_df['Train R²'].tolist()\noverfitting = [train - test for train, test in zip(train_r2_list, test_r2_list)]\n\naxes[0, 1].bar(models_list, overfitting, alpha=0.7, color='red')\naxes[0, 1].set_title('Overfitting Analysis (Train R² - Test R²)')\naxes[0, 1].set_ylabel('Overfitting Score')\naxes[0, 1].tick_params(axis='x', rotation=45)\naxes[0, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n\n# Prediction accuracy (Random Forest)\nbest_model = results[3]  # Random Forest\ny_pred_best = best_model['predictions']\n\naxes[1, 0].scatter(y_test, y_pred_best, alpha=0.6)\naxes[1, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\naxes[1, 0].set_title(f'Prediction Accuracy: {best_model[\"model_name\"]}')\naxes[1, 0].set_xlabel('Actual CLV ($)')\naxes[1, 0].set_ylabel('Predicted CLV ($)')\n\n# Residuals plot\nresiduals = y_test - y_pred_best\naxes[1, 1].scatter(y_pred_best, residuals, alpha=0.6)\naxes[1, 1].axhline(y=0, color='red', linestyle='--')\naxes[1, 1].set_title('Residuals Plot')\naxes[1, 1].set_xlabel('Predicted CLV ($)')\naxes[1, 1].set_ylabel('Residuals ($)')\n\nplt.tight_layout()\nplt.show()\n\n# Best model interpretation\nbest_model_obj = results[3]['model']  # Random Forest\nfeature_importance = best_model_obj.feature_importances_\n\nprint(f\"\\nFEATURE IMPORTANCE (Random Forest)\")\nprint(\"=\"*50)\nimportance_df = pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': feature_importance\n}).sort_values('Importance', ascending=False)\n\nprint(importance_df)\n\n# Business insights\nprint(f\"\\nBUSINESS INSIGHTS\")\nprint(\"=\"*50)\nprint(f\"• Best model: Random Forest (Test R² = {results[3]['test_r2']:.3f})\")\nprint(f\"• Average prediction error: ${results[3]['test_mae']:.0f}\")\nprint(f\"• Most important factors for CLV:\")\nfor i, row in importance_df.head(3).iterrows():\n    print(f\"  - {row['Feature']}: {row['Importance']:.3f}\")\n\nprint(f\"\\n• Overfitting analysis:\")\nfor result in results:\n    overfitting = result['train_r2'] - result['test_r2']\n    status = \"Good\" if overfitting < 0.1 else \"Concerning\" if overfitting < 0.2 else \"Severe\"\n    print(f\"  - {result['model_name']}: {overfitting:.3f} ({status})\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Prepare data for machine learning\n# Convert categorical variables and split into features/target\n\ndef prepare_ml_data(df):\n    \"\"\"Prepare data for machine learning\"\"\"\n    \n    # Create dummy variables for categorical features\n    df_ml = pd.get_dummies(df, columns=['segment'], prefix='segment')\n    \n    # Define features (X) and target (y)\n    feature_columns = [\n        'age', 'income', 'months_active', 'avg_monthly_spend', \n        'purchase_frequency', 'email_open_rate', 'social_media_follower',\n        'segment_Budget', 'segment_Premium', 'segment_Standard'\n    ]\n    \n    X = df_ml[feature_columns]\n    y = df_ml['customer_lifetime_value']\n    \n    return X, y, feature_columns\n\n# Prepare the data\nX, y, feature_names = prepare_ml_data(customer_data)\n\nprint(\"MACHINE LEARNING DATA PREPARATION\")\nprint(\"=\"*50)\nprint(f\"Features (X) shape: {X.shape}\")\nprint(f\"Target (y) shape: {y.shape}\")\nprint(f\"\\nFeature columns:\")\nfor i, feature in enumerate(feature_names):\n    print(f\"{i+1:2d}. {feature}\")\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(f\"\\nData splits:\")\nprint(f\"Training set: {X_train.shape[0]} customers\")\nprint(f\"Test set: {X_test.shape[0]} customers\")\n\n# Standardize features for algorithms that are sensitive to scale\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(f\"\\nFeature scaling applied for algorithms sensitive to scale\")\nprint(f\"Original feature ranges:\")\nprint(X_train.describe().loc[['min', 'max']])\n\nprint(f\"\\nScaled feature ranges:\")\nscaled_df = pd.DataFrame(X_train_scaled, columns=feature_names)\nprint(scaled_df.describe().loc[['min', 'max']])",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.pipeline import Pipeline\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Example 1: Customer Lifetime Value Prediction\n# Predict CLV based on customer characteristics and behavior\n\ndef generate_clv_data(n_customers=1000):\n    \"\"\"Generate realistic customer lifetime value dataset\"\"\"\n    np.random.seed(42)\n    \n    # Customer demographics\n    age = np.random.normal(40, 12, n_customers)\n    age = np.clip(age, 18, 80)  # Realistic age range\n    \n    income = np.random.lognormal(10.5, 0.5, n_customers)  # Log-normal income distribution\n    income = np.clip(income, 25000, 200000)\n    \n    # Customer behavior metrics\n    months_active = np.random.exponential(24, n_customers)  # Exponential tenure\n    months_active = np.clip(months_active, 1, 60)\n    \n    avg_monthly_spend = np.random.gamma(2, 50, n_customers)  # Gamma spending distribution\n    avg_monthly_spend = np.clip(avg_monthly_spend, 10, 500)\n    \n    purchase_frequency = np.random.poisson(3, n_customers) + 1  # Monthly purchase frequency\n    \n    # Customer segments (categorical)\n    segments = np.random.choice(['Premium', 'Standard', 'Budget'], n_customers, p=[0.2, 0.5, 0.3])\n    \n    # Marketing engagement\n    email_opens = np.random.beta(2, 5, n_customers)  # Open rate between 0-1\n    social_media = np.random.binomial(1, 0.4, n_customers)  # Binary: follows on social media\n    \n    # Generate CLV with realistic business relationships\n    clv = (\n        50 +  # Base CLV\n        0.002 * income +  # Higher income -> higher CLV\n        15 * months_active +  # Longer tenure -> higher CLV\n        8 * avg_monthly_spend +  # Higher spend -> higher CLV\n        25 * purchase_frequency +  # More frequent purchases -> higher CLV\n        200 * email_opens +  # Engagement -> higher CLV\n        150 * social_media +  # Social media followers -> higher CLV\n        np.where(segments == 'Premium', 300, \n                np.where(segments == 'Standard', 100, 0)) +  # Segment premium\n        np.random.normal(0, 100, n_customers)  # Random noise\n    )\n    \n    # Ensure CLV is positive\n    clv = np.maximum(clv, 50)\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'customer_id': range(1, n_customers + 1),\n        'age': age,\n        'income': income,\n        'months_active': months_active,\n        'avg_monthly_spend': avg_monthly_spend,\n        'purchase_frequency': purchase_frequency,\n        'segment': segments,\n        'email_open_rate': email_opens,\n        'social_media_follower': social_media,\n        'customer_lifetime_value': clv\n    })\n    \n    return df\n\n# Generate customer data\ncustomer_data = generate_clv_data(1000)\n\nprint(\"CUSTOMER LIFETIME VALUE PREDICTION\")\nprint(\"=\"*50)\nprint(f\"Dataset shape: {customer_data.shape}\")\nprint(f\"\\nFirst few rows:\")\nprint(customer_data.head())\n\nprint(f\"\\nDataset statistics:\")\nprint(customer_data.describe())\n\nprint(f\"\\nCustomer segments:\")\nprint(customer_data['segment'].value_counts())\n\n# Data visualization\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# CLV distribution\naxes[0, 0].hist(customer_data['customer_lifetime_value'], bins=30, alpha=0.7, color='skyblue')\naxes[0, 0].set_title('CLV Distribution')\naxes[0, 0].set_xlabel('Customer Lifetime Value ($)')\n\n# CLV by segment\ncustomer_data.boxplot(column='customer_lifetime_value', by='segment', ax=axes[0, 1])\naxes[0, 1].set_title('CLV by Customer Segment')\naxes[0, 1].set_ylabel('CLV ($)')\n\n# Correlation with spending\naxes[0, 2].scatter(customer_data['avg_monthly_spend'], customer_data['customer_lifetime_value'], alpha=0.6)\naxes[0, 2].set_title('CLV vs Monthly Spend')\naxes[0, 2].set_xlabel('Average Monthly Spend ($)')\naxes[0, 2].set_ylabel('CLV ($)')\n\n# Correlation with tenure\naxes[1, 0].scatter(customer_data['months_active'], customer_data['customer_lifetime_value'], alpha=0.6)\naxes[1, 0].set_title('CLV vs Tenure')\naxes[1, 0].set_xlabel('Months Active')\naxes[1, 0].set_ylabel('CLV ($)')\n\n# Income relationship\naxes[1, 1].scatter(customer_data['income'], customer_data['customer_lifetime_value'], alpha=0.6)\naxes[1, 1].set_title('CLV vs Income')\naxes[1, 1].set_xlabel('Income ($)')\naxes[1, 1].set_ylabel('CLV ($)')\n\n# Purchase frequency\naxes[1, 2].scatter(customer_data['purchase_frequency'], customer_data['customer_lifetime_value'], alpha=0.6)\naxes[1, 2].set_title('CLV vs Purchase Frequency')\naxes[1, 2].set_xlabel('Monthly Purchase Frequency')\naxes[1, 2].set_ylabel('CLV ($)')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Supervised Learning: Regression for Business Prediction\n\nSupervised learning uses labeled training data to learn patterns that can predict outcomes for new data. **Regression** predicts continuous numerical values, making it ideal for forecasting sales, prices, revenues, costs, and other quantitative business metrics.\n\n**Key Concepts:**\n- **Features (X):** Input variables used to make predictions (customer age, company size, market conditions)\n- **Target (y):** The outcome we want to predict (sales revenue, customer lifetime value, stock price)\n- **Training Data:** Historical examples with known outcomes\n- **Test Data:** New data used to evaluate model performance\n- **Overfitting:** Model memorizes training data but fails on new data\n- **Generalization:** Model's ability to perform well on unseen data\n\n**Common Regression Algorithms:**\n- **Linear Regression:** Assumes linear relationships between features and target\n- **Polynomial Regression:** Captures nonlinear relationships with polynomial terms\n- **Regularized Regression:** Prevents overfitting with Lasso (L1) and Ridge (L2) penalties\n- **Random Forest:** Ensemble method that combines many decision trees\n- **Neural Networks:** Flexible models that can learn complex nonlinear patterns\n\n**Business Success Metrics:**\n- **Mean Absolute Error (MAE):** Average prediction error in original units\n- **Root Mean Square Error (RMSE):** Penalizes large errors more heavily\n- **R-squared (R²):** Proportion of variance explained by the model\n- **Business Impact:** Revenue gained, costs saved, or efficiency improved",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Introduction to Machine Learning for Business\n\nMachine Learning (ML) is revolutionizing how businesses make decisions, understand customers, and optimize operations. Unlike traditional statistical analysis that focuses on understanding relationships, machine learning emphasizes **prediction** and **pattern recognition** from data.\n\n**What Makes Machine Learning Different:**\n- **Predictive Focus:** Primary goal is accurate prediction on new, unseen data\n- **Pattern Discovery:** Automatically finds complex relationships in data\n- **Scalability:** Handles large datasets with many variables\n- **Adaptability:** Models can learn and improve as new data becomes available\n\n**Key Business Applications:**\n- **Customer Analytics:** Predicting customer behavior, churn, and lifetime value\n- **Marketing Optimization:** Targeting, personalization, and campaign effectiveness\n- **Risk Management:** Credit scoring, fraud detection, and operational risk\n- **Operations:** Demand forecasting, inventory optimization, and quality control\n- **Human Resources:** Talent acquisition, performance prediction, and retention\n- **Financial Analysis:** Algorithmic trading, portfolio optimization, and market analysis\n\n**The Machine Learning Workflow:**\n1. **Problem Definition:** Clearly define the business question and success metrics\n2. **Data Collection:** Gather relevant, high-quality data\n3. **Data Preparation:** Clean, transform, and engineer features\n4. **Model Selection:** Choose appropriate algorithms for the problem type\n5. **Training & Validation:** Fit models and tune hyperparameters\n6. **Evaluation:** Assess performance on unseen data\n7. **Deployment:** Implement the model in business processes\n8. **Monitoring:** Track performance and retrain as needed\n\n**Types of Machine Learning:**\n- **Supervised Learning:** Learning from labeled examples (predictions, classifications)\n- **Unsupervised Learning:** Finding patterns in unlabeled data (clustering, segmentation)  \n- **Reinforcement Learning:** Learning through trial and error (optimization, game playing)",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}