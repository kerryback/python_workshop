{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "### Conclusion\n\nCongratulations! You've learned the fundamentals of pandas, one of the most powerful libraries for data analysis in Python.\n\n**Key Concepts Covered:**\n- **DataFrame Creation** - From dictionaries, lists, and files\n- **Data Inspection** - Shape, info, head, tail, describe\n- **Data Selection** - Columns, rows, boolean indexing\n- **Data Manipulation** - Adding columns, modifying values\n- **Sorting and Ordering** - Single and multiple column sorting\n- **Grouping and Aggregation** - GroupBy operations and statistics\n- **Data Cleaning** - Handling missing values and duplicates\n- **Merging and Joining** - Combining multiple DataFrames\n- **Data I/O** - Reading from and writing to various file formats\n- **Advanced Operations** - String processing, date handling, pivot tables\n\n**Next Steps:**\n- Practice with real datasets from your domain\n- Learn data visualization with matplotlib and seaborn\n- Explore advanced pandas features like custom aggregations\n- Study time series analysis for temporal data\n- Learn integration with machine learning libraries like scikit-learn\n\n**Best Practices:**\n- Always inspect your data with `.info()` and `.head()`\n- Handle missing values explicitly\n- Use descriptive variable names\n- Comment complex operations\n- Validate data types after reading files\n- Use vectorized operations instead of loops when possible\n\nPandas is an incredibly rich library with many more features to discover. The foundation you've built here will serve you well as you tackle real-world data analysis challenges!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Using the text_data DataFrame from above, extract the first names and assign to 'first_names'.\n\n\nexpected_names = ['Alice', 'Bob', 'Charlie', 'Diana']\nassert list(first_names) == expected_names, f\"Should extract first names: {expected_names}\"\nprint(\"Correct! You extracted first names using string operations.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Execute this cell to see advanced pandas operations\n\nprint(\"=== String Operations ===\")\n# Create data with text for string operations\ntext_data = pd.DataFrame({\n    'Name': ['Alice Johnson', 'Bob Smith-Wilson', 'Charlie Brown', 'Diana Lee'],\n    'Email': ['alice@company.com', 'bob@gmail.com', 'charlie@company.com', 'diana@yahoo.com'],\n    'Phone': ['(555) 123-4567', '555-987-6543', '(555) 111-2222', '555.333.4444']\n})\n\nprint(\"1. Original text data:\")\nprint(text_data)\n\n# String operations\ntext_data['First_Name'] = text_data['Name'].str.split().str[0]\ntext_data['Last_Name'] = text_data['Name'].str.split().str[-1]\ntext_data['Domain'] = text_data['Email'].str.split('@').str[1]\ntext_data['Is_Company_Email'] = text_data['Email'].str.contains('company')\n\nprint(\"\\n2. After string operations:\")\nprint(text_data[['Name', 'First_Name', 'Last_Name', 'Domain', 'Is_Company_Email']])\n\n# Extract area code from phone\ntext_data['Area_Code'] = text_data['Phone'].str.extract(r'(\\d{3})')\nprint(\"\\n3. Extracted area codes:\")\nprint(text_data[['Phone', 'Area_Code']])\n\nprint(\"\\n=== Date and Time Operations ===\")\n# Create time series data\ndates = pd.date_range('2024-01-01', periods=10, freq='D')\nsales_ts = pd.DataFrame({\n    'Date': dates,\n    'Sales': np.random.randint(1000, 5000, 10),\n    'Temperature': np.random.normal(20, 5, 10)\n})\n\nprint(\"4. Time series data:\")\nprint(sales_ts.head())\n\n# Date operations\nsales_ts['Year'] = sales_ts['Date'].dt.year\nsales_ts['Month'] = sales_ts['Date'].dt.month\nsales_ts['Weekday'] = sales_ts['Date'].dt.day_name()\nsales_ts['Is_Weekend'] = sales_ts['Date'].dt.dayofweek >= 5\n\nprint(\"\\n5. Date components:\")\nprint(sales_ts[['Date', 'Year', 'Month', 'Weekday', 'Is_Weekend']].head())\n\nprint(\"\\n=== Rolling Window Operations ===\")\n# Rolling averages\nsales_ts['Sales_3day_avg'] = sales_ts['Sales'].rolling(window=3).mean()\nsales_ts['Sales_3day_sum'] = sales_ts['Sales'].rolling(window=3).sum()\n\nprint(\"6. Rolling window calculations:\")\nprint(sales_ts[['Date', 'Sales', 'Sales_3day_avg', 'Sales_3day_sum']])\n\nprint(\"\\n=== Pivot Table Example ===\")\n# Create more complex data for pivot table\npivot_data = pd.DataFrame({\n    'Date': pd.date_range('2024-01-01', periods=20, freq='D'),\n    'Product': np.random.choice(['Laptop', 'Mouse', 'Keyboard'], 20),\n    'Region': np.random.choice(['North', 'South', 'East', 'West'], 20),\n    'Sales': np.random.randint(100, 1000, 20),\n    'Units': np.random.randint(1, 10, 20)\n})\n\n# Create pivot table\npivot_table = pivot_data.pivot_table(\n    values='Sales',\n    index='Product',\n    columns='Region',\n    aggfunc='mean',\n    fill_value=0\n)\n\nprint(\"7. Pivot table (average sales by product and region):\")\nprint(pivot_table)\n\nprint(\"\\n=== Data Reshaping ===\")\n# Wide to long format using melt\nwide_data = pd.DataFrame({\n    'Student': ['Alice', 'Bob', 'Charlie'],\n    'Math': [85, 92, 78],\n    'Science': [88, 85, 95],\n    'English': [92, 88, 82]\n})\n\nprint(\"8. Wide format data:\")\nprint(wide_data)\n\nlong_data = wide_data.melt(\n    id_vars=['Student'],\n    value_vars=['Math', 'Science', 'English'],\n    var_name='Subject',\n    value_name='Score'\n)\n\nprint(\"\\n9. Long format data:\")\nprint(long_data)\n\n# Long to wide using pivot\nwide_again = long_data.pivot(index='Student', columns='Subject', values='Score')\nprint(\"\\n10. Back to wide format:\")\nprint(wide_again)\n\nprint(\"\\n=== Multi-level Indexing ===\")\n# Create hierarchical index\nhierarchical_data = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5, 6],\n    'B': [10, 20, 30, 40, 50, 60],\n    'C': [100, 200, 300, 400, 500, 600]\n})\n\n# Set multi-level index\nhierarchical_data.index = pd.MultiIndex.from_tuples([\n    ('Group1', 'Item1'), ('Group1', 'Item2'), ('Group1', 'Item3'),\n    ('Group2', 'Item1'), ('Group2', 'Item2'), ('Group2', 'Item3')\n], names=['Group', 'Item'])\n\nprint(\"11. Multi-level indexed data:\")\nprint(hierarchical_data)\n\n# Access specific group\nprint(\"\\n12. Group1 data only:\")\nprint(hierarchical_data.loc['Group1'])\n\nprint(\"\\n=== Advanced Filtering ===\")\n# Complex boolean conditions\ncomplex_filter = pivot_data[\n    (pivot_data['Sales'] > 500) & \n    (pivot_data['Product'] == 'Laptop') &\n    (pivot_data['Region'].isin(['North', 'South']))\n]\n\nprint(\"13. Complex filtering (Laptop sales > 500 in North/South):\")\nprint(complex_filter[['Product', 'Region', 'Sales']])\n\n# Query method (alternative syntax)\nquery_result = pivot_data.query('Sales > 500 and Product == \"Laptop\"')\nprint(\"\\n14. Using query method:\")\nprint(query_result[['Product', 'Region', 'Sales']])\n\nprint(\"\\n=== Summary ===\")\nprint(\"Advanced operations enable:\")\nprint(\"- Complex text processing with .str accessor\")\nprint(\"- Time series analysis with .dt accessor\")\nprint(\"- Moving averages and window functions\")\nprint(\"- Data reshaping for different analysis needs\")\nprint(\"- Hierarchical data organization\")\nprint(\"- Sophisticated filtering and querying\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Advanced Pandas Operations\n\nPandas provides many advanced features for complex data analysis tasks.\n\n**String Operations:**\n- **`.str`** accessor for string methods on Series\n- **`.str.contains()`**, **`.str.startswith()`**, **`.str.endswith()`**\n- **`.str.extract()`**, **`.str.replace()`**, **`.str.split()`**\n\n**Date and Time Operations:**\n- **`pd.to_datetime()`** - Convert to datetime\n- **`.dt`** accessor for datetime operations\n- **`.dt.year`**, **`.dt.month`**, **`.dt.day`**, **`.dt.dayname()`**\n\n**Window Functions:**\n- **`.rolling()`** - Rolling window calculations\n- **`.expanding()`** - Expanding window calculations\n- **`.ewm()`** - Exponentially weighted functions\n\n**Pivot Tables and Reshaping:**\n- **`.pivot_table()`** - Create Excel-style pivot tables\n- **`.melt()`** - Transform wide to long format\n- **`.pivot()`** - Reshape data using unique values\n\n**Advanced Indexing:**\n- **`.set_index()`** / **`.reset_index()`** - Modify index\n- **`pd.MultiIndex`** - Hierarchical indexing\n- **`.xs()`** - Cross-section selection",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Convert the sample_data DataFrame to a CSV string (without index) and assign to 'csv_output'.\n\n\nassert 'Date,Product,Sales,Units,Region' in csv_output, \"Should have proper CSV headers\"\nassert 'Laptop' in csv_output and 'Mouse' in csv_output, \"Should contain product data\"\nassert csv_output.count('\\n') == 6, \"Should have header + 5 data rows\"  # Header + 5 rows\nprint(\"Correct! You converted the DataFrame to CSV format.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Execute this cell to see data I/O operations\n\n# Create sample data for demonstration\nsample_data = pd.DataFrame({\n    'Date': pd.date_range('2024-01-01', periods=5, freq='D'),\n    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones'],\n    'Sales': [1500, 25, 75, 300, 120],\n    'Units': [3, 10, 5, 2, 8],\n    'Region': ['North', 'South', 'East', 'West', 'North']\n})\n\nprint(\"=== Sample Data for I/O Operations ===\")\nprint(sample_data)\n\nprint(\"\\n=== Writing Data ===\")\n\n# Write to CSV (in memory simulation)\ncsv_string = sample_data.to_csv(index=False)\nprint(\"1. CSV format:\")\nprint(csv_string)\n\n# Write to JSON (in memory simulation)\njson_string = sample_data.to_json(orient='records', date_format='iso', indent=2)\nprint(\"2. JSON format:\")\nprint(json_string)\n\n# Different CSV options\ncsv_with_index = sample_data.to_csv()\nprint(\"3. CSV with index:\")\nprint(csv_with_index)\n\n# Custom separator\ncsv_semicolon = sample_data.to_csv(sep=';', index=False)\nprint(\"4. CSV with semicolon separator:\")\nprint(csv_semicolon)\n\nprint(\"\\n=== Reading Data Simulation ===\")\n# Simulate reading from CSV string\nfrom io import StringIO\n\ncsv_data = \"\"\"Product,Price,Category\nLaptop,999.99,Electronics\nBook,19.99,Education\nCoffee,4.50,Food\nPhone,699.99,Electronics\"\"\"\n\ndf_from_csv = pd.read_csv(StringIO(csv_data))\nprint(\"5. Read from CSV:\")\nprint(df_from_csv)\n\n# Read with custom options\ncsv_semicolon_data = \"\"\"Product;Price;Category\nTablet;299.99;Electronics\nPen;1.99;Office\nTea;3.25;Food\"\"\"\n\ndf_semicolon = pd.read_csv(StringIO(csv_semicolon_data), sep=';')\nprint(\"\\n6. Read CSV with semicolon separator:\")\nprint(df_semicolon)\n\n# Read with missing values\ncsv_with_missing = \"\"\"Name,Age,Salary\nAlice,25,70000\nBob,,65000\nCharlie,35,\nDiana,28,60000\"\"\"\n\ndf_missing = pd.read_csv(StringIO(csv_with_missing))\nprint(\"\\n7. Read CSV with missing values:\")\nprint(df_missing)\nprint(\"Data types:\")\nprint(df_missing.dtypes)\n\n# Read with custom NA values\ndf_custom_na = pd.read_csv(StringIO(csv_with_missing), na_values=['', 'N/A', 'NULL'])\nprint(\"\\n8. After handling missing values:\")\nprint(df_custom_na)\n\nprint(\"\\n=== JSON Operations ===\")\njson_data = '''[\n    {\"name\": \"Alice\", \"age\": 25, \"city\": \"New York\"},\n    {\"name\": \"Bob\", \"age\": 30, \"city\": \"London\"},\n    {\"name\": \"Charlie\", \"age\": 35, \"city\": \"Tokyo\"}\n]'''\n\ndf_from_json = pd.read_json(StringIO(json_data))\nprint(\"9. Read from JSON:\")\nprint(df_from_json)\n\n# Different JSON orientations\njson_records = sample_data.to_json(orient='records')\njson_index = sample_data.to_json(orient='index')\njson_values = sample_data.to_json(orient='values')\n\nprint(\"\\n10. JSON orientations:\")\nprint(\"Records:\", json_records[:100] + \"...\")\nprint(\"Index:\", json_index[:100] + \"...\")\nprint(\"Values:\", json_values[:100] + \"...\")\n\nprint(\"\\n=== Data Type Handling ===\")\n# Create data with specific types for writing\ntyped_data = pd.DataFrame({\n    'ID': [1, 2, 3, 4],\n    'Name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n    'Score': [85.5, 92.0, 78.5, 88.0],\n    'Passed': [True, True, False, True],\n    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04'])\n})\n\nprint(\"11. Original data types:\")\nprint(typed_data.dtypes)\n\n# Export and see how types are preserved\ncsv_output = typed_data.to_csv(index=False)\ndf_read_back = pd.read_csv(StringIO(csv_output))\n\nprint(\"\\n12. Data types after CSV round-trip:\")\nprint(df_read_back.dtypes)\n\n# Parse dates during reading\ndf_with_dates = pd.read_csv(StringIO(csv_output), parse_dates=['Date'])\nprint(\"\\n13. Data types with date parsing:\")\nprint(df_with_dates.dtypes)\n\nprint(\"\\n=== Summary ===\")\nprint(\"Key points for data I/O:\")\nprint(\"- CSV is most common but doesn't preserve data types\")\nprint(\"- Use parse_dates for date columns when reading CSV\")\nprint(\"- JSON preserves more structure but can be larger\")\nprint(\"- Always check data types after reading\")\nprint(\"- Consider Parquet for better performance and type preservation\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Reading and Writing Data\n\nPandas excels at reading data from various file formats and writing processed data back to files.\n\n**Reading Data:**\n- **`pd.read_csv()`** - CSV files\n- **`pd.read_excel()`** - Excel files\n- **`pd.read_json()`** - JSON files\n- **`pd.read_sql()`** - SQL databases\n- **`pd.read_parquet()`** - Parquet files\n- **`pd.read_html()`** - HTML tables\n\n**Writing Data:**\n- **`.to_csv()`** - Export to CSV\n- **`.to_excel()`** - Export to Excel\n- **`.to_json()`** - Export to JSON\n- **`.to_sql()`** - Export to SQL database\n- **`.to_parquet()`** - Export to Parquet\n\n**Common Parameters:**\n- **`index`** - Include/exclude row index\n- **`header`** - Include/exclude column headers\n- **`sep`** - Delimiter for CSV files\n- **`encoding`** - File encoding (utf-8, latin-1, etc.)\n- **`na_values`** - Custom missing value indicators",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Merge employees_basic with departments DataFrames using a left join on 'Department'.\n# Assign the result to 'merged_emp_dept'.\n\n\nassert len(merged_emp_dept) == 5, \"Should have 5 rows (all employees)\"\nassert 'Budget' in merged_emp_dept.columns, \"Should include Budget column from departments\"\nassert merged_emp_dept.loc[merged_emp_dept['Name'] == 'Alice', 'Budget'].iloc[0] == 500000, \"Alice should have Engineering budget\"\nprint(\"Correct! You merged the DataFrames using a left join.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Execute this cell to see merging and joining techniques\n\n# Create sample DataFrames for demonstration\nemployees_basic = pd.DataFrame({\n    'EmployeeID': [1, 2, 3, 4, 5],\n    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n    'Department': ['Engineering', 'Marketing', 'Engineering', 'HR', 'Marketing']\n})\n\nemployee_details = pd.DataFrame({\n    'EmployeeID': [1, 2, 3, 6, 7],\n    'Salary': [70000, 65000, 80000, 75000, 68000],\n    'Manager': ['Frank', 'Grace', 'Frank', 'Grace', 'Frank']\n})\n\ndepartments = pd.DataFrame({\n    'Department': ['Engineering', 'Marketing', 'HR', 'Finance'],\n    'Budget': [500000, 300000, 200000, 400000],\n    'Location': ['Building A', 'Building B', 'Building C', 'Building A']\n})\n\nprint(\"=== Sample DataFrames ===\")\nprint(\"Employees Basic:\")\nprint(employees_basic)\nprint(\"\\nEmployee Details:\")\nprint(employee_details)\nprint(\"\\nDepartments:\")\nprint(departments)\n\nprint(\"\\n=== Inner Join ===\")\n# Inner join - only matching records\ninner_join = pd.merge(employees_basic, employee_details, on='EmployeeID', how='inner')\nprint(\"1. Inner join on EmployeeID:\")\nprint(inner_join)\n\nprint(\"\\n=== Left Join ===\")\n# Left join - all records from left DataFrame\nleft_join = pd.merge(employees_basic, employee_details, on='EmployeeID', how='left')\nprint(\"2. Left join on EmployeeID:\")\nprint(left_join)\n\nprint(\"\\n=== Right Join ===\")\n# Right join - all records from right DataFrame\nright_join = pd.merge(employees_basic, employee_details, on='EmployeeID', how='right')\nprint(\"3. Right join on EmployeeID:\")\nprint(right_join)\n\nprint(\"\\n=== Outer Join ===\")\n# Outer join - all records from both DataFrames\nouter_join = pd.merge(employees_basic, employee_details, on='EmployeeID', how='outer')\nprint(\"4. Outer join on EmployeeID:\")\nprint(outer_join)\n\nprint(\"\\n=== Joining on Different Column Names ===\")\n# Create DataFrame with different column name\nemployee_info = pd.DataFrame({\n    'EmpID': [1, 2, 3],\n    'Years_Experience': [2, 5, 8],\n    'Performance_Rating': ['A', 'B', 'A']\n})\n\ndifferent_cols = pd.merge(employees_basic, employee_info, \n                         left_on='EmployeeID', right_on='EmpID', how='inner')\nprint(\"5. Join on different column names:\")\nprint(different_cols)\n\nprint(\"\\n=== Multiple Column Join ===\")\n# Join on multiple columns\nperformance_data = pd.DataFrame({\n    'EmployeeID': [1, 2, 3, 1, 2, 3],\n    'Department': ['Engineering', 'Marketing', 'Engineering', 'Engineering', 'Marketing', 'Engineering'],\n    'Quarter': ['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n    'Sales': [10000, 8000, 12000, 11000, 8500, 13000]\n})\n\nmulti_join = pd.merge(employees_basic, performance_data, \n                     on=['EmployeeID', 'Department'], how='inner')\nprint(\"6. Join on multiple columns:\")\nprint(multi_join)\n\nprint(\"\\n=== Concatenation ===\")\n# Concatenate DataFrames vertically (row-wise)\nnew_employees = pd.DataFrame({\n    'EmployeeID': [6, 7, 8],\n    'Name': ['Frank', 'Grace', 'Henry'],\n    'Department': ['Finance', 'Finance', 'IT']\n})\n\nconcatenated = pd.concat([employees_basic, new_employees], ignore_index=True)\nprint(\"7. Vertical concatenation:\")\nprint(concatenated)\n\n# Concatenate horizontally (column-wise)\nemployee_scores = pd.DataFrame({\n    'Skill_Score': [85, 92, 78, 88, 95],\n    'Team_Score': [90, 85, 95, 82, 88]\n})\n\nhorizontal_concat = pd.concat([employees_basic, employee_scores], axis=1)\nprint(\"\\n8. Horizontal concatenation:\")\nprint(horizontal_concat)\n\nprint(\"\\n=== Handling Overlapping Columns ===\")\n# DataFrames with overlapping column names\nemp_salary = pd.DataFrame({\n    'EmployeeID': [1, 2, 3],\n    'Name': ['Alice Smith', 'Bob Johnson', 'Charlie Brown'],  # Different names\n    'Salary': [70000, 65000, 80000]\n})\n\noverlap_merge = pd.merge(employees_basic, emp_salary, on='EmployeeID', \n                        suffixes=('_basic', '_salary'))\nprint(\"9. Merge with overlapping columns:\")\nprint(overlap_merge)\n\nprint(\"\\n=== Join Method ===\")\n# Using join method (joins on index)\nindexed_employees = employees_basic.set_index('EmployeeID')\nindexed_details = employee_details.set_index('EmployeeID')\n\njoined = indexed_employees.join(indexed_details, how='inner')\nprint(\"10. Using join method:\")\nprint(joined)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Merging and Joining DataFrames\n\nCombining data from multiple DataFrames is a common task in data analysis. Pandas provides several methods for merging and joining data.\n\n**Primary Methods:**\n- **`pd.merge()`** - SQL-style joins with flexible options\n- **`pd.concat()`** - Concatenate along rows or columns\n- **`.join()`** - Join on index (left join by default)\n\n**Types of Joins:**\n- **Inner join**: Only matching rows from both DataFrames\n- **Left join**: All rows from left DataFrame, matching from right\n- **Right join**: All rows from right DataFrame, matching from left\n- **Outer join**: All rows from both DataFrames\n\n**Key Parameters:**\n- **`on`** - Column(s) to join on\n- **`left_on`** / **`right_on`** - Different column names\n- **`how`** - Type of join ('inner', 'left', 'right', 'outer')\n- **`suffixes`** - Suffix for overlapping column names",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Using the messy_data DataFrame above, count the total number of missing values across all columns.\n# Assign the result to 'total_missing'.\n\n\nassert total_missing == 4, \"Should have 4 total missing values (1 Age, 1 Salary, 1 Department, 1 duplicate count)\"\nprint(\"Correct! You counted the total missing values.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Execute this cell to see data cleaning techniques\n\n# Create a messy dataset for demonstration\nmessy_data = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Alice', 'Frank', 'Grace'],\n    'Age': [25, np.nan, 35, 28, 32, 25, 45, 29],\n    'Salary': ['70000', '65000', np.nan, '60000', '72000', '70000', '90000', 'invalid'],\n    'Department': ['Engineering', 'Marketing', 'Engineering', None, 'Marketing', 'Engineering', 'Engineering', 'HR'],\n    'Join_Date': ['2022-01-15', '2021-03-20', '2020-06-10', '2023-02-28', '2021-11-05', '2022-01-15', '2019-04-12', '2023-05-30']\n})\n\nprint(\"=== Original Messy Data ===\")\nprint(messy_data)\nprint(f\"\\nData types:\\n{messy_data.dtypes}\")\n\nprint(\"\\n=== Missing Value Detection ===\")\n# Check for missing values\nprint(\"1. Missing values per column:\")\nprint(messy_data.isnull().sum())\n\nprint(\"\\n2. Rows with any missing values:\")\nprint(messy_data[messy_data.isnull().any(axis=1)])\n\nprint(\"\\n3. Missing value pattern:\")\nprint(messy_data.isnull())\n\nprint(\"\\n=== Handling Missing Values ===\")\n# Drop rows with any missing values\nclean_dropped = messy_data.dropna()\nprint(\"4. After dropping rows with missing values:\")\nprint(clean_dropped)\n\n# Fill missing values\nclean_filled = messy_data.copy()\nclean_filled['Age'].fillna(clean_filled['Age'].mean(), inplace=True)\nclean_filled['Department'].fillna('Unknown', inplace=True)\nclean_filled['Salary'].fillna('0', inplace=True)\nprint(\"\\n5. After filling missing values:\")\nprint(clean_filled[['Name', 'Age', 'Department', 'Salary']])\n\nprint(\"\\n=== Duplicate Handling ===\")\n# Identify duplicates\nprint(\"6. Duplicate rows:\")\nprint(messy_data.duplicated())\n\nprint(\"\\n7. Rows that are duplicates:\")\nprint(messy_data[messy_data.duplicated()])\n\n# Remove duplicates\nno_duplicates = messy_data.drop_duplicates()\nprint(\"\\n8. After removing duplicates:\")\nprint(no_duplicates)\n\nprint(\"\\n=== Data Type Conversion ===\")\n# Convert Salary to numeric, handling errors\nclean_data = messy_data.copy()\nclean_data['Salary'] = pd.to_numeric(clean_data['Salary'], errors='coerce')\nprint(\"9. After converting Salary to numeric:\")\nprint(clean_data[['Name', 'Salary']])\nprint(f\"Salary type: {clean_data['Salary'].dtype}\")\n\n# Convert Join_Date to datetime\nclean_data['Join_Date'] = pd.to_datetime(clean_data['Join_Date'])\nprint(\"\\n10. After converting Join_Date to datetime:\")\nprint(clean_data[['Name', 'Join_Date']])\nprint(f\"Join_Date type: {clean_data['Join_Date'].dtype}\")\n\nprint(\"\\n=== Advanced Cleaning ===\")\n# Remove duplicates but keep last occurrence\nno_dups_last = messy_data.drop_duplicates(keep='last')\nprint(\"11. Remove duplicates (keep last):\")\nprint(no_dups_last)\n\n# Fill missing values with forward fill\nforward_filled = messy_data.copy()\nforward_filled['Department'] = forward_filled['Department'].ffill()\nprint(\"\\n12. Forward fill Department:\")\nprint(forward_filled[['Name', 'Department']])\n\n# Interpolate missing numeric values\ninterpolated = messy_data.copy()\ninterpolated['Age'] = pd.to_numeric(interpolated['Age'], errors='coerce')\ninterpolated['Age'] = interpolated['Age'].interpolate()\nprint(\"\\n13. Interpolated Age values:\")\nprint(interpolated[['Name', 'Age']])\n\nprint(\"\\n=== Summary Statistics After Cleaning ===\")\n# Final cleaned dataset\nfinal_clean = messy_data.copy()\nfinal_clean = final_clean.drop_duplicates()\nfinal_clean['Age'] = pd.to_numeric(final_clean['Age'], errors='coerce')\nfinal_clean['Salary'] = pd.to_numeric(final_clean['Salary'], errors='coerce')\nfinal_clean['Join_Date'] = pd.to_datetime(final_clean['Join_Date'])\nfinal_clean = final_clean.dropna()\n\nprint(\"14. Final cleaned data info:\")\nfinal_clean.info()\nprint(\"\\n15. Final cleaned data:\")\nprint(final_clean)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Data Cleaning and Missing Values\n\nReal-world data is often messy with missing values, duplicates, and inconsistencies. Pandas provides comprehensive tools for data cleaning.\n\n**Missing Value Detection:**\n- **`.isnull()`** / **`.isna()`** - Check for missing values\n- **`.notnull()`** / **`.notna()`** - Check for non-missing values\n- **`.info()`** - Shows non-null counts for each column\n\n**Handling Missing Values:**\n- **`.dropna()`** - Remove rows/columns with missing values\n- **`.fillna()`** - Fill missing values with specified values\n- **`.interpolate()`** - Fill missing values using interpolation\n- **`.ffill()`** / **`.bfill()`** - Forward/backward fill\n\n**Duplicate Handling:**\n- **`.duplicated()`** - Identify duplicate rows\n- **`.drop_duplicates()`** - Remove duplicate rows\n\n**Data Type Conversion:**\n- **`.astype()`** - Convert column data types\n- **`pd.to_numeric()`**, **`pd.to_datetime()`** - Specialized conversions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Group the employees DataFrame by Department and find the maximum Years of experience.\n# Assign the result to 'max_years_by_dept'.\n\n\nassert max_years_by_dept['Engineering'] == 12, \"Engineering should have max 12 years\"\nassert max_years_by_dept['Marketing'] == 6, \"Marketing should have max 6 years\"\nprint(\"Correct! You found the maximum years of experience by department.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Execute this cell to see grouping and aggregation\n\nprint(\"Original employees DataFrame:\")\nprint(employees)\n\nprint(\"\\n=== Basic GroupBy Operations ===\")\n# Group by department and calculate average salary\ndept_avg_salary = employees.groupby('Department')['Salary'].mean()\nprint(\"1. Average salary by department:\")\nprint(dept_avg_salary)\n\n# Multiple aggregations\ndept_stats = employees.groupby('Department')['Salary'].agg(['mean', 'min', 'max', 'count'])\nprint(\"\\n2. Multiple salary statistics by department:\")\nprint(dept_stats)\n\nprint(\"\\n=== Multiple Column Aggregations ===\")\n# Aggregate multiple columns\ndept_summary = employees.groupby('Department').agg({\n    'Salary': ['mean', 'sum'],\n    'Age': 'mean',\n    'Years': 'max'\n})\nprint(\"3. Multiple column aggregations:\")\nprint(dept_summary)\n\nprint(\"\\n=== Group Sizes and Counts ===\")\n# Count employees in each department\ndept_counts = employees.groupby('Department').size()\nprint(\"4. Number of employees per department:\")\nprint(dept_counts)\n\n# Count non-null values\ndept_count_values = employees.groupby('Department').count()\nprint(\"\\n5. Count of non-null values by department:\")\nprint(dept_count_values)\n\nprint(\"\\n=== Custom Aggregations ===\")\n# Define custom function\ndef salary_range(series):\n    return series.max() - series.min()\n\ndept_custom = employees.groupby('Department')['Salary'].agg([\n    ('Average', 'mean'),\n    ('Total', 'sum'),\n    ('Range', salary_range)\n])\nprint(\"6. Custom aggregations:\")\nprint(dept_custom)\n\nprint(\"\\n=== Multiple Grouping Columns ===\")\n# Create more complex data for demonstration\ncomplex_data = pd.DataFrame({\n    'Region': ['North', 'North', 'South', 'South', 'North', 'South'] * 2,\n    'Department': ['Engineering', 'Marketing', 'Engineering', 'Marketing', 'HR', 'HR'] * 2,\n    'Sales': [1000, 800, 1200, 900, 600, 700, 1100, 850, 1300, 950, 650, 750],\n    'Quarter': ['Q1', 'Q1', 'Q1', 'Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2', 'Q2', 'Q2', 'Q2']\n})\n\nprint(\"7. Complex data for multi-level grouping:\")\nprint(complex_data.head(8))\n\n# Group by multiple columns\nmulti_group = complex_data.groupby(['Region', 'Department'])['Sales'].agg(['mean', 'sum'])\nprint(\"\\n8. Multi-level grouping (Region and Department):\")\nprint(multi_group)\n\nprint(\"\\n=== Transform and Apply ===\")\n# Transform: add group statistics to original data\nemployees['Dept_Avg_Salary'] = employees.groupby('Department')['Salary'].transform('mean')\nprint(\"9. Added department average salary to each row:\")\nprint(employees[['Name', 'Department', 'Salary', 'Dept_Avg_Salary']])\n\n# Apply custom function to groups\ndef dept_analysis(group):\n    return pd.Series({\n        'count': len(group),\n        'avg_salary': group['Salary'].mean(),\n        'senior_employees': (group['Years'] >= 5).sum()\n    })\n\ndept_analysis_result = employees.groupby('Department').apply(dept_analysis)\nprint(\"\\n10. Custom department analysis:\")\nprint(dept_analysis_result)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Grouping and Aggregation\n\nGroupBy operations are among the most powerful features in pandas, allowing you to split data into groups, apply functions to each group, and combine the results.\n\n**GroupBy Workflow:**\n1. **Split**: Divide data into groups based on criteria\n2. **Apply**: Perform operations on each group\n3. **Combine**: Merge results back together\n\n**Basic GroupBy Syntax:**\n```python\ndf.groupby('column').agg_function()\ndf.groupby(['col1', 'col2']).agg_function()\n```\n\n**Common Aggregation Functions:**\n- **`.mean()`**, **`.sum()`**, **`.count()`**\n- **`.min()`**, **`.max()`**, **`.std()`**\n- **`.agg()`** - Apply multiple functions\n- **`.apply()`** - Apply custom functions\n\n**Advanced GroupBy:**\n- **`.size()`** - Count rows in each group\n- **`.nunique()`** - Count unique values\n- **`.transform()`** - Return same-shaped data\n- **`.filter()`** - Filter groups based on criteria",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sort the employees DataFrame by Age in descending order and assign to 'sorted_by_age'.\n\n\nassert sorted_by_age.iloc[0]['Name'] == 'Frank', \"Frank should be first (oldest)\"\nassert sorted_by_age.iloc[-1]['Name'] == 'Alice', \"Alice should be last (youngest)\"\nprint(\"Correct! You sorted the DataFrame by age in descending order.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Execute this cell to see sorting techniques\n\nprint(\"Original employees DataFrame:\")\nprint(employees)\n\nprint(\"\\n=== Single Column Sorting ===\")\n# Sort by salary (ascending)\nsalary_asc = employees.sort_values('Salary')\nprint(\"1. Sorted by Salary (ascending):\")\nprint(salary_asc[['Name', 'Salary']])\n\n# Sort by age (descending)\nage_desc = employees.sort_values('Age', ascending=False)\nprint(\"\\n2. Sorted by Age (descending):\")\nprint(age_desc[['Name', 'Age']])\n\nprint(\"\\n=== Multiple Column Sorting ===\")\n# Sort by Department first, then by Salary within each department\ndept_salary = employees.sort_values(['Department', 'Salary'], ascending=[True, False])\nprint(\"3. Sorted by Department, then Salary (desc):\")\nprint(dept_salary[['Name', 'Department', 'Salary']])\n\nprint(\"\\n=== Top and Bottom Values ===\")\n# Get top 3 earners\ntop_earners = employees.nlargest(3, 'Salary')\nprint(\"4. Top 3 earners:\")\nprint(top_earners[['Name', 'Salary']])\n\n# Get youngest 2 employees\nyoungest = employees.nsmallest(2, 'Age')\nprint(\"\\n5. Youngest 2 employees:\")\nprint(youngest[['Name', 'Age']])\n\nprint(\"\\n=== Sorting by Index ===\")\n# Create a DataFrame with custom index\nindexed_df = employees.set_index('Name')\nprint(\"6. DataFrame with Name as index:\")\nprint(indexed_df.head(3))\n\n# Sort by index (alphabetically by name)\nname_sorted = indexed_df.sort_index()\nprint(\"\\n7. Sorted by index (Name):\")\nprint(name_sorted[['Age', 'Department']])\n\nprint(\"\\n=== Advanced Sorting ===\")\n# Sort by string length of Department name\ndept_length = employees.sort_values('Department', key=lambda x: x.str.len())\nprint(\"8. Sorted by Department name length:\")\nprint(dept_length[['Name', 'Department']])\n\n# Create a DataFrame with missing values for demonstration\nemp_with_nan = employees.copy()\nemp_with_nan.loc[1, 'Salary'] = np.nan\nemp_with_nan.loc[4, 'Age'] = np.nan\n\nprint(\"\\n9. DataFrame with NaN values:\")\nprint(emp_with_nan)\n\n# Sort with NaN handling\nnan_last = emp_with_nan.sort_values('Salary', na_position='last')\nprint(\"\\n10. Sorted with NaN values last:\")\nprint(nan_last[['Name', 'Salary']])",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Sorting and Ordering Data\n\nSorting data is essential for analysis and presentation. Pandas provides flexible sorting capabilities.\n\n**Sorting Methods:**\n- **`.sort_values()`** - Sort by one or more columns\n- **`.sort_index()`** - Sort by row index\n- **`.nlargest()`** / **`.nsmallest()`** - Get top/bottom n rows\n\n**Key Parameters:**\n- **`by`** - Column(s) to sort by\n- **`ascending`** - True (default) for ascending, False for descending\n- **`na_position`** - Where to put NaN values ('first' or 'last')\n- **`inplace`** - Modify original DataFrame (default False)\n- **`key`** - Function to apply to values before sorting\n\n**Multiple Column Sorting:**\n```python\ndf.sort_values(['col1', 'col2'], ascending=[True, False])\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Using the original employees DataFrame, add a new column 'Total_Compensation' \n# that equals Salary + (Salary * 0.15). Assign the modified DataFrame to 'emp_with_total'.\n\n\nassert 'Total_Compensation' in emp_with_total.columns, \"Should have Total_Compensation column\"\nassert emp_with_total.loc[0, 'Total_Compensation'] == 70000 * 1.15, \"Should calculate total compensation correctly\"\nprint(\"Correct! You added a calculated column.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Execute this cell to see data manipulation techniques\n\n# Start with a copy of our employees DataFrame\nemp_df = employees.copy()\nprint(\"Original DataFrame:\")\nprint(emp_df)\n\nprint(\"\\n=== Adding New Columns ===\")\n# Calculate annual bonus (10% of salary)\nemp_df['Bonus'] = emp_df['Salary'] * 0.10\nprint(\"1. Added Bonus column:\")\nprint(emp_df[['Name', 'Salary', 'Bonus']])\n\n# Add experience level based on years\nemp_df['Experience_Level'] = np.where(emp_df['Years'] < 5, 'Junior', \n                                    np.where(emp_df['Years'] < 10, 'Senior', 'Expert'))\nprint(\"\\n2. Added Experience Level:\")\nprint(emp_df[['Name', 'Years', 'Experience_Level']])\n\n# Add a categorical column\nemp_df['Salary_Category'] = pd.cut(emp_df['Salary'], \n                                  bins=[0, 65000, 75000, float('inf')], \n                                  labels=['Low', 'Medium', 'High'])\nprint(\"\\n3. Added Salary Category:\")\nprint(emp_df[['Name', 'Salary', 'Salary_Category']])\n\nprint(\"\\n=== Modifying Existing Data ===\")\n# Give everyone a 5% raise\nemp_df['Salary'] = emp_df['Salary'] * 1.05\nprint(\"4. After 5% salary increase:\")\nprint(emp_df[['Name', 'Salary']])\n\n# Update specific values\nemp_df.loc[emp_df['Name'] == 'Alice', 'Department'] = 'Data Science'\nprint(\"\\n5. Alice moved to Data Science:\")\nprint(emp_df[emp_df['Name'] == 'Alice'])\n\nprint(\"\\n=== Adding New Rows ===\")\n# Add a new employee\nnew_employee = ['Grace', 29, 'Finance', 68000, 4, 6800, 'Junior', 'Medium']\nemp_df.loc[len(emp_df)] = new_employee\nprint(\"6. Added new employee:\")\nprint(emp_df.tail(2))\n\nprint(\"\\n=== Column Operations ===\")\n# Rename columns\nemp_df_renamed = emp_df.rename(columns={'Years': 'Years_Experience', 'Bonus': 'Annual_Bonus'})\nprint(\"7. Renamed columns:\")\nprint(list(emp_df_renamed.columns))\n\n# Drop columns\nemp_df_reduced = emp_df.drop(columns=['Bonus', 'Salary_Category'])\nprint(\"\\n8. After dropping columns:\")\nprint(list(emp_df_reduced.columns))\n\n# Reorder columns\ncolumn_order = ['Name', 'Department', 'Age', 'Years', 'Salary', 'Experience_Level']\nemp_df_reordered = emp_df[column_order]\nprint(\"\\n9. Reordered columns:\")\nprint(emp_df_reordered.head(3))\n\nprint(\"\\n=== Replace Values ===\")\n# Replace department names\nemp_df['Department'] = emp_df['Department'].replace({\n    'Engineering': 'Tech',\n    'Marketing': 'Sales & Marketing'\n})\nprint(\"10. After replacing department names:\")\nprint(emp_df['Department'].unique())",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Data Manipulation and Modification\n\nPandas provides extensive capabilities for modifying and manipulating data within DataFrames.\n\n**Adding and Modifying Columns:**\n- **New column**: `df['new_col'] = values`\n- **Calculated column**: `df['new_col'] = df['col1'] + df['col2']`\n- **Conditional column**: `df['new_col'] = np.where(condition, value1, value2)`\n\n**Adding and Removing Rows:**\n- **Add row**: `df.loc[new_index] = values`\n- **Concatenate**: `pd.concat([df1, df2])`\n- **Drop rows**: `df.drop(index_labels)`\n\n**Modifying Values:**\n- **Single value**: `df.loc[row, col] = new_value`\n- **Multiple values**: `df.loc[condition, 'column'] = new_value`\n- **Replace values**: `df.replace(old_value, new_value)`\n\n**Column Operations:**\n- **Rename columns**: `df.rename(columns={'old': 'new'})`\n- **Drop columns**: `df.drop(columns=['col1', 'col2'])`\n- **Reorder columns**: `df[['col2', 'col1', 'col3']]`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Select only the 'Name' and 'Department' columns from the employees DataFrame.\n# Assign the result to 'name_dept'.\n\n\nassert list(name_dept.columns) == ['Name', 'Department'], \"Should only have Name and Department columns\"\nassert len(name_dept) == 6, \"Should have all 6 employees\"\nprint(\"Correct! You selected specific columns.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Using the employees DataFrame above, select employees who are older than 30.\n# Assign the result to 'older_employees'.\n\n\nassert len(older_employees) == 3, \"Should have 3 employees older than 30\"\nassert all(older_employees['Age'] > 30), \"All selected employees should be older than 30\"\nprint(\"Correct! You filtered employees by age.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Execute this cell to see data selection methods\n\n# Create a sample DataFrame for examples\nemployees = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank'],\n    'Age': [25, 30, 35, 28, 32, 45],\n    'Department': ['Engineering', 'Marketing', 'Engineering', 'HR', 'Marketing', 'Engineering'],\n    'Salary': [70000, 65000, 80000, 60000, 72000, 90000],\n    'Years': [2, 5, 8, 3, 6, 12]\n})\n\nprint(\"Sample DataFrame:\")\nprint(employees)\nprint()\n\nprint(\"=== Column Selection ===\")\n# Single column (returns Series)\nprint(\"1. Single column (Name):\")\nprint(employees['Name'])\nprint(f\"Type: {type(employees['Name'])}\")\n\nprint(\"\\n2. Single column using dot notation:\")\nprint(employees.Department.head(3))\n\n# Multiple columns (returns DataFrame)\nprint(\"\\n3. Multiple columns:\")\nprint(employees[['Name', 'Salary']])\n\nprint(\"\\n=== Row Selection ===\")\n# Single row by position\nprint(\"4. Single row by position (iloc):\")\nprint(employees.iloc[0])  # First row\n\nprint(\"\\n5. Multiple rows by position:\")\nprint(employees.iloc[1:4])  # Rows 1, 2, 3\n\nprint(\"\\n6. Multiple rows by index labels:\")\nprint(employees.loc[0:2])  # Rows 0, 1, 2 (inclusive)\n\nprint(\"\\n=== Cell Selection ===\")\nprint(\"7. Specific cell:\")\nprint(f\"Alice's salary: {employees.loc[0, 'Salary']}\")\nprint(f\"Same using iloc: {employees.iloc[0, 3]}\")\n\nprint(\"\\n8. Multiple cells:\")\nprint(employees.loc[0:2, ['Name', 'Age']])\n\nprint(\"\\n=== Boolean Indexing (Filtering) ===\")\nprint(\"9. Employees with salary > 70000:\")\nhigh_earners = employees[employees['Salary'] > 70000]\nprint(high_earners)\n\nprint(\"\\n10. Multiple conditions (Engineering dept with salary > 70000):\")\neng_high = employees[(employees['Department'] == 'Engineering') & (employees['Salary'] > 70000)]\nprint(eng_high)\n\nprint(\"\\n11. Using isin() for multiple values:\")\nmarketing_hr = employees[employees['Department'].isin(['Marketing', 'HR'])]\nprint(marketing_hr)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Selecting and Accessing Data\n\nPandas provides multiple ways to select and access data from DataFrames, similar to how you might work with spreadsheets or databases.\n\n**Column Selection:**\n- **Single column**: `df['column_name']` or `df.column_name`\n- **Multiple columns**: `df[['col1', 'col2']]`\n\n**Row Selection:**\n- **By index**: `df.loc[row_label]` or `df.iloc[row_number]`\n- **By range**: `df.loc[start:end]` or `df.iloc[start:end]`\n\n**Cell Selection:**\n- **Specific cell**: `df.loc[row, column]` or `df.iloc[row_num, col_num]`\n- **Multiple cells**: `df.loc[rows, columns]`\n\n**Boolean Indexing:**\n- **Filter rows**: `df[df['column'] > value]`\n- **Multiple conditions**: `df[(df['col1'] > val1) & (df['col2'] == val2)]`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Using the sales_df from above, find the total number of rows and columns.\n# Assign the number of rows to 'num_rows' and number of columns to 'num_cols'.\n\n\nassert num_rows == 24 and num_cols == 5, \"Should have 24 rows and 5 columns\"\nprint(\"Correct! You extracted the DataFrame dimensions.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Execute this cell to explore DataFrame attributes and information\n\n# Create a larger dataset for better examples\nnp.random.seed(42)  # For reproducible random numbers\nsales_data = {\n    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones', 'Tablet', 'Phone', 'Speaker'] * 3,\n    'Quarter': ['Q1', 'Q1', 'Q1', 'Q1', 'Q1', 'Q1', 'Q1', 'Q1'] * 2 + ['Q2'] * 8,\n    'Sales': np.random.randint(1000, 10000, 24),\n    'Units': np.random.randint(10, 100, 24),\n    'Region': np.random.choice(['North', 'South', 'East', 'West'], 24)\n}\nsales_df = pd.DataFrame(sales_data)\n\nprint(\"=== DataFrame Attributes ===\\n\")\n\nprint(\"1. Basic Shape and Size:\")\nprint(f\"Shape: {sales_df.shape}\")\nprint(f\"Size: {sales_df.size}\")\nprint(f\"Dimensions: {sales_df.ndim}\")\n\nprint(\"\\n2. Column and Index Information:\")\nprint(f\"Columns: {list(sales_df.columns)}\")\nprint(f\"Index: {sales_df.index.tolist()}\")\nprint(f\"Column types:\\n{sales_df.dtypes}\")\n\nprint(\"\\n3. First few rows:\")\nprint(sales_df.head(3))\n\nprint(\"\\n4. Last few rows:\")\nprint(sales_df.tail(3))\n\nprint(\"\\n5. Random sample:\")\nprint(sales_df.sample(3))\n\nprint(\"\\n6. Comprehensive info:\")\nsales_df.info()\n\nprint(\"\\n7. Statistical summary:\")\nprint(sales_df.describe())",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### DataFrame Attributes and Basic Information\n\nDataFrames have many useful attributes that help you understand your data structure and content.\n\n**Essential Attributes:**\n- **`.shape`** - Returns (rows, columns)\n- **`.size`** - Total number of elements\n- **`.ndim`** - Number of dimensions (always 2 for DataFrames)\n- **`.columns`** - Column names\n- **`.index`** - Row labels\n- **`.dtypes`** - Data types of each column\n- **`.values`** - Underlying NumPy array\n\n**Information Methods:**\n- **`.info()`** - Comprehensive overview of the DataFrame\n- **`.describe()`** - Statistical summary of numeric columns\n- **`.head(n)`** - First n rows (default 5)\n- **`.tail(n)`** - Last n rows (default 5)\n- **`.sample(n)`** - Random sample of n rows",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create a DataFrame with student information using a dictionary.\n# Include columns: 'Student', 'Math', 'Science', 'English' with data for 3 students.\n\n\nassert len(students_df) == 3, \"Should have 3 students\"\nassert list(students_df.columns) == ['Student', 'Math', 'Science', 'English'], \"Should have correct column names\"\nprint(\"Correct! You created a DataFrame with student information.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Execute this cell to see different ways of creating DataFrames\n\nprint(\"=== Creating DataFrames ===\\n\")\n\n# Method 1: From dictionary (most common)\ndict_data = {\n    'Product': ['Laptop', 'Mouse', 'Keyboard'],\n    'Price': [999.99, 29.99, 79.99],\n    'Stock': [10, 50, 25]\n}\ndf1 = pd.DataFrame(dict_data)\nprint(\"1. From Dictionary:\")\nprint(df1)\n\n# Method 2: From list of lists\nlist_data = [\n    ['Apple', 'Fruit', 1.50],\n    ['Carrot', 'Vegetable', 0.75],\n    ['Bread', 'Bakery', 2.25]\n]\ndf2 = pd.DataFrame(list_data, columns=['Item', 'Category', 'Price'])\nprint(\"\\n2. From List of Lists:\")\nprint(df2)\n\n# Method 3: From list of dictionaries\ndict_list = [\n    {'Name': 'John', 'Score': 85, 'Grade': 'B'},\n    {'Name': 'Jane', 'Score': 92, 'Grade': 'A'},\n    {'Name': 'Bob', 'Score': 78, 'Grade': 'C'}\n]\ndf3 = pd.DataFrame(dict_list)\nprint(\"\\n3. From List of Dictionaries:\")\nprint(df3)\n\n# Method 4: With custom index\ndf4 = pd.DataFrame(\n    {'Temperature': [20, 25, 30, 22], 'Humidity': [60, 65, 70, 55]},\n    index=['Monday', 'Tuesday', 'Wednesday', 'Thursday']\n)\nprint(\"\\n4. With Custom Index:\")\nprint(df4)\n\n# Method 5: Empty DataFrame (useful for building data iteratively)\ndf5 = pd.DataFrame(columns=['A', 'B', 'C'])\nprint(\"\\n5. Empty DataFrame:\")\nprint(df5)\nprint(f\"Shape: {df5.shape}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Creating DataFrames\n\nThere are several ways to create pandas DataFrames depending on your data source and format.\n\n**Common Methods:**\n1. **From Dictionary**: `pd.DataFrame(dict)`\n2. **From List of Lists**: `pd.DataFrame(data, columns=column_names)`\n3. **From CSV File**: `pd.read_csv('file.csv')`\n4. **From Excel File**: `pd.read_excel('file.xlsx')`\n5. **From JSON**: `pd.read_json('file.json')`\n6. **Empty DataFrame**: `pd.DataFrame()`\n\n**Key Parameters:**\n- `columns`: Specify column names\n- `index`: Specify row labels\n- `dtype`: Specify data types for columns",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Execute this cell to import pandas and create our first DataFrame\n\nimport pandas as pd\nimport numpy as np\n\n# Create a simple DataFrame from a dictionary\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n    'Age': [25, 30, 35, 28, 32],\n    'City': ['New York', 'London', 'Tokyo', 'Paris', 'Sydney'],\n    'Salary': [70000, 80000, 90000, 75000, 85000],\n    'Department': ['Engineering', 'Marketing', 'Engineering', 'HR', 'Marketing']\n}\n\ndf = pd.DataFrame(data)\nprint(\"Our first DataFrame:\")\nprint(df)\nprint(f\"\\nDataFrame type: {type(df)}\")\nprint(f\"Shape: {df.shape}\")  # (rows, columns)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<div style=\"background-image: url('https://www.dropbox.com/scl/fi/wdrnuojbnjx6lgfekrx85/mcnair.jpg?rlkey=wcbaw5au7vh5vt1g5d5x7fw8f&dl=1'); background-size: cover; background-position: center; height: 300px; display: flex; align-items: center; justify-content: center; color: white; text-shadow: 2px 2px 4px rgba(0,0,0,0.7); margin-bottom: 20px; position: relative;\">\n  <h1 style=\"text-align: center; font-size: 2.5em; margin: 0;\">JGSB Python Workshop <br> Part 7: Data Tables</h1>\n  <div style=\"position: absolute; bottom: 10px; left: 15px; font-size: 0.9em; color: white; text-shadow: 2px 2px 4px rgba(0,0,0,0.7);\">\n    Authored by Kerry Back\n  </div>\n  <div style=\"position: absolute; bottom: 10px; right: 15px; text-align: right; font-size: 0.9em; color: white; text-shadow: 2px 2px 4px rgba(0,0,0,0.7);\">\n    Rice University, 9/6/2025\n  </div>\n</div>",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Introduction to Pandas\n\n**Pandas** is the most important library for data analysis in Python. It provides powerful, flexible data structures that make working with structured data intuitive and efficient.\n\n**Key Features:**\n- **DataFrame**: 2D labeled data structure (like a spreadsheet or SQL table)\n- **Series**: 1D labeled array (like a column in a spreadsheet)\n- **Data Import/Export**: Read from CSV, Excel, SQL, JSON, and many other formats\n- **Data Cleaning**: Handle missing data, duplicates, and data type conversions\n- **Data Manipulation**: Filter, sort, group, merge, and transform data\n- **Statistical Analysis**: Built-in statistical functions and operations\n\n**Why Pandas?**\n- Works seamlessly with other Python libraries (NumPy, Matplotlib, scikit-learn)\n- Handles real-world messy data gracefully\n- Intuitive syntax similar to SQL and Excel\n- Excellent performance for medium-sized datasets\n- Extensive documentation and community support\n\n**Installation:**\n```python\npip install pandas\n```\n\n**Import Convention:**\n```python\nimport pandas as pd\n```",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}